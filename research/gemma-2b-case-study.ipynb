{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribution Case Study: Gemma - 2B\n",
    "\n",
    "In this notebook we show how feature attribution (as calculated by our open source library!) can be used to understand model behaviour and help hypothesise reasons behind undesirable outputs. We showcase a case study using the Gemma-2B model and our gradient-based perturbation method described here. \n",
    "\n",
    "Since Gemma-2B is an open-source model, we will be downloading a local copy of it before calculating attribution values. To access Gemma-2B through huggingface, a huggingface account with approval is necessary. Please click [here](https://huggingface.co/google/gemma-2b) to go through the approval process. After approval, the huggingface account needs to be logged into using the CLI command `huggingface-cli login`.\n",
    "\n",
    "We will use our `LocalLLMAttributor` function that employs a gradient-based perturbation technique to calculate attribution values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "sys.path.append(str(Path(os.getcwd()).parent))\n",
    "from attribution.local_attribution import LocalLLMAttributor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Gemma-2 model, tokenizer and embeddings\n",
    "\n",
    "Make sure you've logged in using the `huggingface-cli login` command and have access to the Gemma-2B model hosted [here](https://huggingface.co/google/gemma-2b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6502d7d693e4c23a65b94eab6d77d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_id = \"google/gemma-2b-it\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\").cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "embeddings = model.get_input_embeddings().weight.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 1: Apples to Apples\n",
    "\n",
    "We ask Gemma-2B a simple question: 'Sam has 3 apples and Tom has 5 oranges. How many apples does Sam have?'. Gemma-2B correctly answers this question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: <bos>Sam has 3 apples and Tom has 5 oranges. How many apples does Sam have?\n",
      "\n",
      "The answer is 3.\n"
     ]
    }
   ],
   "source": [
    "input_string = 'Sam has 3 apples and Tom has 5 oranges. How many apples does Sam have?'\n",
    "input_tokens = tokenizer(input_string, return_tensors='pt').input_ids.to(model.device)\n",
    "output_tokens = model.generate(input_tokens, max_new_tokens=7).squeeze(0)\n",
    "print(f'Output: {tokenizer.decode(output_tokens)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 2: Apples to Oranges?\n",
    "\n",
    "We now slightly change the question to be: 'Sam has 3 apples and Tom has 5 oranges. How many oranges does Sam have?'. Gemma-2B gets this question wrong this time around, choosing to answer 5 oranges even though these oranges belong to Tom! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: <bos>Sam has 3 apples and Tom has 5 oranges. How many oranges does Sam have?\n",
      "\n",
      "The answer is 5.\n"
     ]
    }
   ],
   "source": [
    "input_string = 'Sam has 3 apples and Tom has 5 oranges. How many oranges does Sam have?'\n",
    "input_tokens = tokenizer(input_string, return_tensors='pt').input_ids.to(model.device)\n",
    "output_tokens = model.generate(input_tokens, max_new_tokens=7).squeeze(0)\n",
    "print(f'Output: {tokenizer.decode(output_tokens)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observing Attribution values\n",
    "\n",
    "Could we have predicted this behaviour if we observed the attribution values for Scenario 1? Let's take a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">&lt;bos&gt;Sam▁has▁3▁apples▁and▁John▁has▁5▁oranges.▁How▁many▁apples▁does▁Sam▁have?\n",
       "\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">The▁answer▁is▁3.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<bos>Sam▁has▁3▁apples▁and▁John▁has▁5▁oranges.▁How▁many▁apples▁does▁Sam▁have?\n",
       "\n",
       "\u001b[34mThe\u001b[0m\u001b[34m▁answer\u001b[0m\u001b[34m▁is\u001b[0m\u001b[34m▁\u001b[0m\u001b[34m3\u001b[0m\u001b[34m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                                                                      \n",
       " <span style=\"font-weight: bold\">          </span> <span style=\"font-weight: bold\">        </span> <span style=\"font-weight: bold\">          </span> <span style=\"font-weight: bold\">         </span> <span style=\"font-weight: bold\">          </span> <span style=\"font-weight: bold\">          </span> <span style=\"font-weight: bold\">          </span> <span style=\"font-weight: bold\">          </span> \n",
       " <span style=\"font-weight: bold\">          </span> <span style=\"font-weight: bold\">        </span> <span style=\"font-weight: bold\">          </span> <span style=\"font-weight: bold\">         </span> <span style=\"font-weight: bold\">          </span> <span style=\"font-weight: bold\">          </span> <span style=\"font-weight: bold\">          </span> <span style=\"font-weight: bold\">          </span> \n",
       " <span style=\"font-weight: bold\">          </span> <span style=\"font-weight: bold\">        </span> <span style=\"font-weight: bold\">      The </span> <span style=\"font-weight: bold\"> ▁answer </span> <span style=\"font-weight: bold\">      ▁is </span> <span style=\"font-weight: bold\">        ▁ </span> <span style=\"font-weight: bold\">        3 </span> <span style=\"font-weight: bold\">        . </span> \n",
       " ──────────────────────────────────────────────────────────────────────────────────── \n",
       "  &lt;bos&gt;      <span style=\"color: #008000; text-decoration-color: #008000\">0.7480</span>   <span style=\"color: #800000; text-decoration-color: #800000\">827.3525</span>    <span style=\"color: #008000; text-decoration-color: #008000\">2.4268</span>   <span style=\"color: #808000; text-decoration-color: #808000\">231.6353</span>   <span style=\"color: #800000; text-decoration-color: #800000\">551.3010</span>   <span style=\"color: #800000; text-decoration-color: #800000\">460.8656</span>   <span style=\"color: #800000; text-decoration-color: #800000\">488.2078</span>  \n",
       "  Sam        <span style=\"color: #008000; text-decoration-color: #008000\">0.1883</span>   <span style=\"color: #008000; text-decoration-color: #008000\">185.5456</span>    <span style=\"color: #008000; text-decoration-color: #008000\">1.8554</span>    <span style=\"color: #008000; text-decoration-color: #008000\">38.9216</span>   <span style=\"color: #008000; text-decoration-color: #008000\">104.9274</span>    <span style=\"color: #008000; text-decoration-color: #008000\">24.2310</span>   <span style=\"color: #008000; text-decoration-color: #008000\">122.5083</span>  \n",
       "  ▁has       <span style=\"color: #008000; text-decoration-color: #008000\">0.0620</span>    <span style=\"color: #008000; text-decoration-color: #008000\">57.6189</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.4312</span>     <span style=\"color: #008000; text-decoration-color: #008000\">8.8093</span>    <span style=\"color: #008000; text-decoration-color: #008000\">41.0717</span>    <span style=\"color: #008000; text-decoration-color: #008000\">38.5860</span>    <span style=\"color: #008000; text-decoration-color: #008000\">33.7193</span>  \n",
       "  ▁          <span style=\"color: #008000; text-decoration-color: #008000\">0.0340</span>    <span style=\"color: #008000; text-decoration-color: #008000\">26.2205</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.2369</span>     <span style=\"color: #008000; text-decoration-color: #008000\">4.0255</span>    <span style=\"color: #008000; text-decoration-color: #008000\">24.5529</span>    <span style=\"color: #008000; text-decoration-color: #008000\">27.5611</span>    <span style=\"color: #008000; text-decoration-color: #008000\">18.0878</span>  \n",
       "  3          <span style=\"color: #008000; text-decoration-color: #008000\">0.0337</span>    <span style=\"color: #008000; text-decoration-color: #008000\">28.9997</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.2823</span>     <span style=\"color: #008000; text-decoration-color: #008000\">5.0299</span>    <span style=\"color: #008000; text-decoration-color: #008000\">29.9640</span>    <span style=\"color: #008000; text-decoration-color: #008000\">59.1844</span>    <span style=\"color: #008000; text-decoration-color: #008000\">32.6388</span>  \n",
       "  ▁apples    <span style=\"color: #008000; text-decoration-color: #008000\">0.0632</span>    <span style=\"color: #008000; text-decoration-color: #008000\">66.3430</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.4825</span>     <span style=\"color: #008000; text-decoration-color: #008000\">9.2379</span>    <span style=\"color: #008000; text-decoration-color: #008000\">43.1779</span>    <span style=\"color: #008000; text-decoration-color: #008000\">48.0342</span>    <span style=\"color: #008000; text-decoration-color: #008000\">40.1680</span>  \n",
       "  ▁and       <span style=\"color: #008000; text-decoration-color: #008000\">0.0410</span>    <span style=\"color: #008000; text-decoration-color: #008000\">35.8075</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.3035</span>     <span style=\"color: #008000; text-decoration-color: #008000\">5.8184</span>    <span style=\"color: #008000; text-decoration-color: #008000\">28.4838</span>    <span style=\"color: #008000; text-decoration-color: #008000\">29.5762</span>    <span style=\"color: #008000; text-decoration-color: #008000\">19.1980</span>  \n",
       "  ▁John      <span style=\"color: #008000; text-decoration-color: #008000\">0.0550</span>    <span style=\"color: #008000; text-decoration-color: #008000\">36.0833</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.4106</span>     <span style=\"color: #008000; text-decoration-color: #008000\">7.2349</span>    <span style=\"color: #008000; text-decoration-color: #008000\">24.1436</span>    <span style=\"color: #008000; text-decoration-color: #008000\">21.1167</span>    <span style=\"color: #008000; text-decoration-color: #008000\">19.9228</span>  \n",
       "  ▁has       <span style=\"color: #008000; text-decoration-color: #008000\">0.0457</span>    <span style=\"color: #008000; text-decoration-color: #008000\">33.2646</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.3197</span>     <span style=\"color: #008000; text-decoration-color: #008000\">6.0864</span>    <span style=\"color: #008000; text-decoration-color: #008000\">27.5308</span>    <span style=\"color: #008000; text-decoration-color: #008000\">27.2985</span>    <span style=\"color: #008000; text-decoration-color: #008000\">20.2047</span>  \n",
       "  ▁          <span style=\"color: #008000; text-decoration-color: #008000\">0.0342</span>    <span style=\"color: #008000; text-decoration-color: #008000\">18.9863</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.2035</span>     <span style=\"color: #008000; text-decoration-color: #008000\">3.3157</span>    <span style=\"color: #008000; text-decoration-color: #008000\">20.6291</span>    <span style=\"color: #008000; text-decoration-color: #008000\">17.4662</span>    <span style=\"color: #008000; text-decoration-color: #008000\">14.2829</span>  \n",
       "  5          <span style=\"color: #008000; text-decoration-color: #008000\">0.0317</span>    <span style=\"color: #008000; text-decoration-color: #008000\">25.6511</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.2331</span>     <span style=\"color: #008000; text-decoration-color: #008000\">3.8841</span>    <span style=\"color: #008000; text-decoration-color: #008000\">17.9062</span>    <span style=\"color: #008000; text-decoration-color: #008000\">18.8697</span>    <span style=\"color: #008000; text-decoration-color: #008000\">20.8133</span>  \n",
       "  ▁oranges   <span style=\"color: #008000; text-decoration-color: #008000\">0.0598</span>    <span style=\"color: #008000; text-decoration-color: #008000\">44.3548</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.3668</span>     <span style=\"color: #008000; text-decoration-color: #008000\">6.8322</span>    <span style=\"color: #008000; text-decoration-color: #008000\">33.3165</span>    <span style=\"color: #008000; text-decoration-color: #008000\">41.9764</span>    <span style=\"color: #008000; text-decoration-color: #008000\">27.7359</span>  \n",
       "  .          <span style=\"color: #008000; text-decoration-color: #008000\">0.0435</span>    <span style=\"color: #008000; text-decoration-color: #008000\">32.0238</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.3410</span>     <span style=\"color: #008000; text-decoration-color: #008000\">6.2816</span>    <span style=\"color: #008000; text-decoration-color: #008000\">24.9733</span>    <span style=\"color: #008000; text-decoration-color: #008000\">17.1649</span>    <span style=\"color: #008000; text-decoration-color: #008000\">18.4730</span>  \n",
       "  ▁How       <span style=\"color: #008000; text-decoration-color: #008000\">0.0459</span>    <span style=\"color: #008000; text-decoration-color: #008000\">36.6674</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.3877</span>     <span style=\"color: #008000; text-decoration-color: #008000\">7.0877</span>    <span style=\"color: #008000; text-decoration-color: #008000\">29.7497</span>    <span style=\"color: #008000; text-decoration-color: #008000\">22.6589</span>    <span style=\"color: #008000; text-decoration-color: #008000\">22.1241</span>  \n",
       "  ▁many      <span style=\"color: #008000; text-decoration-color: #008000\">0.0470</span>    <span style=\"color: #008000; text-decoration-color: #008000\">36.2684</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.3264</span>     <span style=\"color: #008000; text-decoration-color: #008000\">6.4464</span>    <span style=\"color: #008000; text-decoration-color: #008000\">33.1295</span>    <span style=\"color: #008000; text-decoration-color: #008000\">30.9442</span>    <span style=\"color: #008000; text-decoration-color: #008000\">27.3642</span>  \n",
       "  ▁apples    <span style=\"color: #008000; text-decoration-color: #008000\">0.0716</span>    <span style=\"color: #008000; text-decoration-color: #008000\">43.8999</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.3341</span>     <span style=\"color: #008000; text-decoration-color: #008000\">7.3740</span>    <span style=\"color: #008000; text-decoration-color: #008000\">46.5694</span>    <span style=\"color: #008000; text-decoration-color: #008000\">51.8812</span>    <span style=\"color: #008000; text-decoration-color: #008000\">32.5115</span>  \n",
       "  ▁does      <span style=\"color: #008000; text-decoration-color: #008000\">0.0323</span>    <span style=\"color: #008000; text-decoration-color: #008000\">33.0140</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.2630</span>     <span style=\"color: #008000; text-decoration-color: #008000\">4.6643</span>    <span style=\"color: #008000; text-decoration-color: #008000\">23.4358</span>    <span style=\"color: #008000; text-decoration-color: #008000\">19.6655</span>    <span style=\"color: #008000; text-decoration-color: #008000\">14.9890</span>  \n",
       "  ▁Sam       <span style=\"color: #008000; text-decoration-color: #008000\">0.0409</span>    <span style=\"color: #008000; text-decoration-color: #008000\">27.3865</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.3168</span>     <span style=\"color: #008000; text-decoration-color: #008000\">4.7084</span>    <span style=\"color: #008000; text-decoration-color: #008000\">28.0450</span>    <span style=\"color: #008000; text-decoration-color: #008000\">26.5090</span>    <span style=\"color: #008000; text-decoration-color: #008000\">17.9461</span>  \n",
       "  ▁have      <span style=\"color: #008000; text-decoration-color: #008000\">0.0649</span>    <span style=\"color: #008000; text-decoration-color: #008000\">34.1126</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.2902</span>     <span style=\"color: #008000; text-decoration-color: #008000\">6.5851</span>    <span style=\"color: #008000; text-decoration-color: #008000\">35.5255</span>    <span style=\"color: #008000; text-decoration-color: #008000\">22.1672</span>    <span style=\"color: #008000; text-decoration-color: #008000\">23.9016</span>  \n",
       "  ?          <span style=\"color: #008000; text-decoration-color: #008000\">0.1469</span>    <span style=\"color: #008000; text-decoration-color: #008000\">59.1601</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.5917</span>    <span style=\"color: #008000; text-decoration-color: #008000\">11.7581</span>    <span style=\"color: #008000; text-decoration-color: #008000\">68.4922</span>    <span style=\"color: #008000; text-decoration-color: #008000\">40.9595</span>    <span style=\"color: #008000; text-decoration-color: #008000\">41.5126</span>  \n",
       "             <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>   <span style=\"color: #008000; text-decoration-color: #008000\">150.9977</span>    <span style=\"color: #008000; text-decoration-color: #008000\">1.2861</span>    <span style=\"color: #008000; text-decoration-color: #008000\">19.3280</span>    <span style=\"color: #008000; text-decoration-color: #008000\">45.8290</span>    <span style=\"color: #008000; text-decoration-color: #008000\">32.2954</span>    <span style=\"color: #008000; text-decoration-color: #008000\">41.1756</span>  \n",
       "                                                                                      \n",
       "                                                                                      \n",
       "  The        <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>     <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>    <span style=\"color: #008000; text-decoration-color: #008000\">1.2992</span>     <span style=\"color: #008000; text-decoration-color: #008000\">8.3744</span>    <span style=\"color: #008000; text-decoration-color: #008000\">20.3927</span>     <span style=\"color: #008000; text-decoration-color: #008000\">8.5602</span>    <span style=\"color: #008000; text-decoration-color: #008000\">16.5280</span>  \n",
       "  ▁answer    <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>     <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>    <span style=\"color: #008000; text-decoration-color: #008000\">27.1139</span>    <span style=\"color: #008000; text-decoration-color: #008000\">38.1941</span>    <span style=\"color: #008000; text-decoration-color: #008000\">18.5473</span>    <span style=\"color: #008000; text-decoration-color: #008000\">29.7542</span>  \n",
       "  ▁is        <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>     <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>     <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>    <span style=\"color: #008000; text-decoration-color: #008000\">94.0511</span>    <span style=\"color: #008000; text-decoration-color: #008000\">27.8277</span>    <span style=\"color: #008000; text-decoration-color: #008000\">47.6029</span>  \n",
       "  ▁          <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>     <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>     <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>     <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>    <span style=\"color: #008000; text-decoration-color: #008000\">57.5610</span>    <span style=\"color: #008000; text-decoration-color: #008000\">14.0093</span>  \n",
       "  3          <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>     <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>     <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>     <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>     <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>    <span style=\"color: #008000; text-decoration-color: #008000\">48.1393</span>  \n",
       "  .          <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>     <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>     <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>     <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>     <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>     <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>  \n",
       "                                                                                      \n",
       "</pre>\n"
      ],
      "text/plain": [
       "                                                                                      \n",
       " \u001b[1m          \u001b[0m \u001b[1m \u001b[0m\u001b[1m      \u001b[0m\u001b[1m \u001b[0m \u001b[1m          \u001b[0m \u001b[1m         \u001b[0m \u001b[1m          \u001b[0m \u001b[1m          \u001b[0m \u001b[1m          \u001b[0m \u001b[1m          \u001b[0m \n",
       " \u001b[1m          \u001b[0m \u001b[1m \u001b[0m\u001b[1m      \u001b[0m\u001b[1m \u001b[0m \u001b[1m          \u001b[0m \u001b[1m         \u001b[0m \u001b[1m          \u001b[0m \u001b[1m          \u001b[0m \u001b[1m          \u001b[0m \u001b[1m          \u001b[0m \n",
       " \u001b[1m \u001b[0m\u001b[1m        \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m      \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m     The\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m▁answer\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m     ▁is\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m       ▁\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m       3\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m       .\u001b[0m\u001b[1m \u001b[0m \n",
       " ──────────────────────────────────────────────────────────────────────────────────── \n",
       "  <bos>      \u001b[32m0.7480\u001b[0m   \u001b[31m827.3525\u001b[0m    \u001b[32m2.4268\u001b[0m   \u001b[33m231.6353\u001b[0m   \u001b[31m551.3010\u001b[0m   \u001b[31m460.8656\u001b[0m   \u001b[31m488.2078\u001b[0m  \n",
       "  Sam        \u001b[32m0.1883\u001b[0m   \u001b[32m185.5456\u001b[0m    \u001b[32m1.8554\u001b[0m    \u001b[32m38.9216\u001b[0m   \u001b[32m104.9274\u001b[0m    \u001b[32m24.2310\u001b[0m   \u001b[32m122.5083\u001b[0m  \n",
       "  ▁has       \u001b[32m0.0620\u001b[0m    \u001b[32m57.6189\u001b[0m    \u001b[32m0.4312\u001b[0m     \u001b[32m8.8093\u001b[0m    \u001b[32m41.0717\u001b[0m    \u001b[32m38.5860\u001b[0m    \u001b[32m33.7193\u001b[0m  \n",
       "  ▁          \u001b[32m0.0340\u001b[0m    \u001b[32m26.2205\u001b[0m    \u001b[32m0.2369\u001b[0m     \u001b[32m4.0255\u001b[0m    \u001b[32m24.5529\u001b[0m    \u001b[32m27.5611\u001b[0m    \u001b[32m18.0878\u001b[0m  \n",
       "  3          \u001b[32m0.0337\u001b[0m    \u001b[32m28.9997\u001b[0m    \u001b[32m0.2823\u001b[0m     \u001b[32m5.0299\u001b[0m    \u001b[32m29.9640\u001b[0m    \u001b[32m59.1844\u001b[0m    \u001b[32m32.6388\u001b[0m  \n",
       "  ▁apples    \u001b[32m0.0632\u001b[0m    \u001b[32m66.3430\u001b[0m    \u001b[32m0.4825\u001b[0m     \u001b[32m9.2379\u001b[0m    \u001b[32m43.1779\u001b[0m    \u001b[32m48.0342\u001b[0m    \u001b[32m40.1680\u001b[0m  \n",
       "  ▁and       \u001b[32m0.0410\u001b[0m    \u001b[32m35.8075\u001b[0m    \u001b[32m0.3035\u001b[0m     \u001b[32m5.8184\u001b[0m    \u001b[32m28.4838\u001b[0m    \u001b[32m29.5762\u001b[0m    \u001b[32m19.1980\u001b[0m  \n",
       "  ▁John      \u001b[32m0.0550\u001b[0m    \u001b[32m36.0833\u001b[0m    \u001b[32m0.4106\u001b[0m     \u001b[32m7.2349\u001b[0m    \u001b[32m24.1436\u001b[0m    \u001b[32m21.1167\u001b[0m    \u001b[32m19.9228\u001b[0m  \n",
       "  ▁has       \u001b[32m0.0457\u001b[0m    \u001b[32m33.2646\u001b[0m    \u001b[32m0.3197\u001b[0m     \u001b[32m6.0864\u001b[0m    \u001b[32m27.5308\u001b[0m    \u001b[32m27.2985\u001b[0m    \u001b[32m20.2047\u001b[0m  \n",
       "  ▁          \u001b[32m0.0342\u001b[0m    \u001b[32m18.9863\u001b[0m    \u001b[32m0.2035\u001b[0m     \u001b[32m3.3157\u001b[0m    \u001b[32m20.6291\u001b[0m    \u001b[32m17.4662\u001b[0m    \u001b[32m14.2829\u001b[0m  \n",
       "  5          \u001b[32m0.0317\u001b[0m    \u001b[32m25.6511\u001b[0m    \u001b[32m0.2331\u001b[0m     \u001b[32m3.8841\u001b[0m    \u001b[32m17.9062\u001b[0m    \u001b[32m18.8697\u001b[0m    \u001b[32m20.8133\u001b[0m  \n",
       "  ▁oranges   \u001b[32m0.0598\u001b[0m    \u001b[32m44.3548\u001b[0m    \u001b[32m0.3668\u001b[0m     \u001b[32m6.8322\u001b[0m    \u001b[32m33.3165\u001b[0m    \u001b[32m41.9764\u001b[0m    \u001b[32m27.7359\u001b[0m  \n",
       "  .          \u001b[32m0.0435\u001b[0m    \u001b[32m32.0238\u001b[0m    \u001b[32m0.3410\u001b[0m     \u001b[32m6.2816\u001b[0m    \u001b[32m24.9733\u001b[0m    \u001b[32m17.1649\u001b[0m    \u001b[32m18.4730\u001b[0m  \n",
       "  ▁How       \u001b[32m0.0459\u001b[0m    \u001b[32m36.6674\u001b[0m    \u001b[32m0.3877\u001b[0m     \u001b[32m7.0877\u001b[0m    \u001b[32m29.7497\u001b[0m    \u001b[32m22.6589\u001b[0m    \u001b[32m22.1241\u001b[0m  \n",
       "  ▁many      \u001b[32m0.0470\u001b[0m    \u001b[32m36.2684\u001b[0m    \u001b[32m0.3264\u001b[0m     \u001b[32m6.4464\u001b[0m    \u001b[32m33.1295\u001b[0m    \u001b[32m30.9442\u001b[0m    \u001b[32m27.3642\u001b[0m  \n",
       "  ▁apples    \u001b[32m0.0716\u001b[0m    \u001b[32m43.8999\u001b[0m    \u001b[32m0.3341\u001b[0m     \u001b[32m7.3740\u001b[0m    \u001b[32m46.5694\u001b[0m    \u001b[32m51.8812\u001b[0m    \u001b[32m32.5115\u001b[0m  \n",
       "  ▁does      \u001b[32m0.0323\u001b[0m    \u001b[32m33.0140\u001b[0m    \u001b[32m0.2630\u001b[0m     \u001b[32m4.6643\u001b[0m    \u001b[32m23.4358\u001b[0m    \u001b[32m19.6655\u001b[0m    \u001b[32m14.9890\u001b[0m  \n",
       "  ▁Sam       \u001b[32m0.0409\u001b[0m    \u001b[32m27.3865\u001b[0m    \u001b[32m0.3168\u001b[0m     \u001b[32m4.7084\u001b[0m    \u001b[32m28.0450\u001b[0m    \u001b[32m26.5090\u001b[0m    \u001b[32m17.9461\u001b[0m  \n",
       "  ▁have      \u001b[32m0.0649\u001b[0m    \u001b[32m34.1126\u001b[0m    \u001b[32m0.2902\u001b[0m     \u001b[32m6.5851\u001b[0m    \u001b[32m35.5255\u001b[0m    \u001b[32m22.1672\u001b[0m    \u001b[32m23.9016\u001b[0m  \n",
       "  ?          \u001b[32m0.1469\u001b[0m    \u001b[32m59.1601\u001b[0m    \u001b[32m0.5917\u001b[0m    \u001b[32m11.7581\u001b[0m    \u001b[32m68.4922\u001b[0m    \u001b[32m40.9595\u001b[0m    \u001b[32m41.5126\u001b[0m  \n",
       "             \u001b[32m0.0000\u001b[0m   \u001b[32m150.9977\u001b[0m    \u001b[32m1.2861\u001b[0m    \u001b[32m19.3280\u001b[0m    \u001b[32m45.8290\u001b[0m    \u001b[32m32.2954\u001b[0m    \u001b[32m41.1756\u001b[0m  \n",
       "                                                                                      \n",
       "                                                                                      \n",
       "  The        \u001b[32m0.0000\u001b[0m     \u001b[32m0.0000\u001b[0m    \u001b[32m1.2992\u001b[0m     \u001b[32m8.3744\u001b[0m    \u001b[32m20.3927\u001b[0m     \u001b[32m8.5602\u001b[0m    \u001b[32m16.5280\u001b[0m  \n",
       "  ▁answer    \u001b[32m0.0000\u001b[0m     \u001b[32m0.0000\u001b[0m    \u001b[32m0.0000\u001b[0m    \u001b[32m27.1139\u001b[0m    \u001b[32m38.1941\u001b[0m    \u001b[32m18.5473\u001b[0m    \u001b[32m29.7542\u001b[0m  \n",
       "  ▁is        \u001b[32m0.0000\u001b[0m     \u001b[32m0.0000\u001b[0m    \u001b[32m0.0000\u001b[0m     \u001b[32m0.0000\u001b[0m    \u001b[32m94.0511\u001b[0m    \u001b[32m27.8277\u001b[0m    \u001b[32m47.6029\u001b[0m  \n",
       "  ▁          \u001b[32m0.0000\u001b[0m     \u001b[32m0.0000\u001b[0m    \u001b[32m0.0000\u001b[0m     \u001b[32m0.0000\u001b[0m     \u001b[32m0.0000\u001b[0m    \u001b[32m57.5610\u001b[0m    \u001b[32m14.0093\u001b[0m  \n",
       "  3          \u001b[32m0.0000\u001b[0m     \u001b[32m0.0000\u001b[0m    \u001b[32m0.0000\u001b[0m     \u001b[32m0.0000\u001b[0m     \u001b[32m0.0000\u001b[0m     \u001b[32m0.0000\u001b[0m    \u001b[32m48.1393\u001b[0m  \n",
       "  .          \u001b[32m0.0000\u001b[0m     \u001b[32m0.0000\u001b[0m    \u001b[32m0.0000\u001b[0m     \u001b[32m0.0000\u001b[0m     \u001b[32m0.0000\u001b[0m     \u001b[32m0.0000\u001b[0m     \u001b[32m0.0000\u001b[0m  \n",
       "                                                                                      \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attributor = LocalLLMAttributor(model=model, embeddings=embeddings, tokenizer=tokenizer)\n",
    "attr_scores, token_ids = attributor.compute_attributions(\n",
    "    input_string=\"Sam has 3 apples and John has 5 oranges. How many apples does Sam have?\",\n",
    "    generation_length=7,\n",
    ")\n",
    "\n",
    "attributor.print_attributions(\n",
    "    word_list=tokenizer.convert_ids_to_tokens(token_ids),\n",
    "    attr_scores=attr_scores,\n",
    "    token_ids=token_ids,\n",
    "    generation_length=7,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the attribution table, we see some sensible values associated with the output token `3` which is the correct answer:\n",
    "\n",
    "|Input Token        | Output Token | Attribution |\n",
    "|-------------------|--------------|-------------|\n",
    "|`3`                | `3`          |  59.1844    |\n",
    "|`apples` (First)   | `3`          |  48.0342    |\n",
    "|`apples` (Second)  | `3`          |  51.8812    |\n",
    "|`5`                | `3`          |  18.8697    |\n",
    "\n",
    "There are high attributions for the input tokens that are strongly associated with the correct answer and low attribution for the input token `5` which would be the incorrect answer. There is something strange about some attribution values in this table for the output token `3`, notably the one associated with the`oranges` and both `Sam` input tokens.\n",
    "\n",
    "|Input Token        | Output Token | Attribution |\n",
    "|-------------------|--------------|-------------|\n",
    "|`oranges`          | `3`          |  41.9764    |\n",
    "|`Sam` (First)      | `3`          |  24.2310    |\n",
    "|`Sam` (Second)     | `3`          |  26.5090    |\n",
    "\n",
    "This value for the `oranges` token is surprisingly close to the input tokens associated with the correct answer input tokens and also quite a bit larger than the values associated with the incorrect answer input tokens. The values for both the `Sam` tokens however are surprisingly low and ideally should be higher since the model answers the question in Scenario 1 correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above analysis, we can come up with two hypotheses about model behaviour that could be potential causes for the failure in Scenario 2:\n",
    "\n",
    "__Hypothesis 1__: Gemma-2B pays undue attention on the `oranges` tokens when asked about `apples` in Scenario 1 pointing to some confusion around which fruit is being asked about in the question.\n",
    "\n",
    "__Hypothesis 2__: Gemma-2B doesn't associate `apples` strongly with `Sam` in Scenario 1 which causes the model to output the incorrect answer in Scenario 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
