{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attribution Case Study: Gemma - 2B\n",
    "\n",
    "In this notebook we show how feature attribution (as calculated by our open source library!) can be used to understand model behaviour and help hypothesise reasons behind undesirable outputs. We showcase a case study using the Gemma-2B model and our gradient-based perturbation method described here. \n",
    "\n",
    "Since Gemma-2B is an open-source model, we will be downloading a local copy of it before calculating attribution values. To access Gemma-2B through huggingface, a huggingface account with approval is necessary. Please click [here](https://huggingface.co/google/gemma-2b) to go through the approval process. After approval, the huggingface account needs to be logged into using the CLI command `huggingface-cli login`.\n",
    "\n",
    "We will use our `LocalLLMAttributor` function that employs a gradient-based perturbation technique to calculate attribution values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "sys.path.append(str(Path(os.getcwd()).parent))\n",
    "from attribution.local_attribution import LocalLLMAttributor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Gemma-2 model, tokenizer and embeddings\n",
    "\n",
    "Make sure you've logged in using the `huggingface-cli login` command and have access to the Gemma-2B model hosted [here](https://huggingface.co/google/gemma-2b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6502d7d693e4c23a65b94eab6d77d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_id = \"google/gemma-2b-it\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\").cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "embeddings = model.get_input_embeddings().weight.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 1: Apples to Apples\n",
    "\n",
    "We ask Gemma-2B a simple question: 'Sam has 3 apples and Tom has 5 oranges. How many apples does Sam have?'. Gemma-2B correctly answers this question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: <bos>Sam has 3 apples and Tom has 5 oranges. How many apples does Sam have?\n",
      "\n",
      "The answer is 3.\n"
     ]
    }
   ],
   "source": [
    "input_string = \"Sam has 3 apples and Tom has 5 oranges. How many apples does Sam have?\"\n",
    "input_tokens = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "output_tokens = model.generate(input_tokens, max_new_tokens=7).squeeze(0)\n",
    "print(f\"Output: {tokenizer.decode(output_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario 2: Apples to Oranges?\n",
    "\n",
    "We now slightly change the question to be: 'Sam has 3 apples and Tom has 5 oranges. How many oranges does Sam have?'. Gemma-2B gets this question wrong this time around, choosing to answer 5 oranges even though these oranges belong to Tom! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: <bos>Sam has 3 apples and Tom has 5 oranges. How many oranges does Sam have?\n",
      "\n",
      "The answer is 5.\n"
     ]
    }
   ],
   "source": [
    "input_string = \"Sam has 3 apples and Tom has 5 oranges. How many oranges does Sam have?\"\n",
    "input_tokens = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "output_tokens = model.generate(input_tokens, max_new_tokens=7).squeeze(0)\n",
    "print(f\"Output: {tokenizer.decode(output_tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observing Attribution values\n",
    "\n",
    "Could we have predicted this behaviour if we observed the attribution values for Scenario 1? Let's take a look!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">&lt;bos&gt;Sam▁has▁3▁apples▁and▁John▁has▁5▁oranges.▁How▁many▁apples▁does▁Sam▁have?\n",
       "\n",
       "<span style=\"color: #000080; text-decoration-color: #000080\">The▁answer▁is▁3.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "<bos>Sam▁has▁3▁apples▁and▁John▁has▁5▁oranges.▁How▁many▁apples▁does▁Sam▁have?\n",
       "\n",
       "\u001b[34mThe\u001b[0m\u001b[34m▁answer\u001b[0m\u001b[34m▁is\u001b[0m\u001b[34m▁\u001b[0m\u001b[34m3\u001b[0m\u001b[34m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                                                                      \n",
       " <span style=\"font-weight: bold\">          </span> <span style=\"font-weight: bold\">        </span> <span style=\"font-weight: bold\">          </span> <span style=\"font-weight: bold\">         </span> <span style=\"font-weight: bold\">          </span> <span style=\"font-weight: bold\">          </span> <span style=\"font-weight: bold\">          </span> <span style=\"font-weight: bold\">          </span> \n",
       " <span style=\"font-weight: bold\">          </span> <span style=\"font-weight: bold\">        </span> <span style=\"font-weight: bold\">          </span> <span style=\"font-weight: bold\">         </span> <span style=\"font-weight: bold\">          </span> <span style=\"font-weight: bold\">          </span> <span style=\"font-weight: bold\">          </span> <span style=\"font-weight: bold\">          </span> \n",
       " <span style=\"font-weight: bold\">          </span> <span style=\"font-weight: bold\">        </span> <span style=\"font-weight: bold\">      The </span> <span style=\"font-weight: bold\"> ▁answer </span> <span style=\"font-weight: bold\">      ▁is </span> <span style=\"font-weight: bold\">        ▁ </span> <span style=\"font-weight: bold\">        3 </span> <span style=\"font-weight: bold\">        . </span> \n",
       " ──────────────────────────────────────────────────────────────────────────────────── \n",
       "  &lt;bos&gt;      <span style=\"color: #008000; text-decoration-color: #008000\">0.7480</span>   <span style=\"color: #800000; text-decoration-color: #800000\">827.3525</span>    <span style=\"color: #008000; text-decoration-color: #008000\">2.4268</span>   <span style=\"color: #808000; text-decoration-color: #808000\">231.6353</span>   <span style=\"color: #800000; text-decoration-color: #800000\">551.3010</span>   <span style=\"color: #800000; text-decoration-color: #800000\">460.8656</span>   <span style=\"color: #800000; text-decoration-color: #800000\">488.2078</span>  \n",
       "  Sam        <span style=\"color: #008000; text-decoration-color: #008000\">0.1883</span>   <span style=\"color: #008000; text-decoration-color: #008000\">185.5456</span>    <span style=\"color: #008000; text-decoration-color: #008000\">1.8554</span>    <span style=\"color: #008000; text-decoration-color: #008000\">38.9216</span>   <span style=\"color: #008000; text-decoration-color: #008000\">104.9274</span>    <span style=\"color: #008000; text-decoration-color: #008000\">24.2310</span>   <span style=\"color: #008000; text-decoration-color: #008000\">122.5083</span>  \n",
       "  ▁has       <span style=\"color: #008000; text-decoration-color: #008000\">0.0620</span>    <span style=\"color: #008000; text-decoration-color: #008000\">57.6189</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.4312</span>     <span style=\"color: #008000; text-decoration-color: #008000\">8.8093</span>    <span style=\"color: #008000; text-decoration-color: #008000\">41.0717</span>    <span style=\"color: #008000; text-decoration-color: #008000\">38.5860</span>    <span style=\"color: #008000; text-decoration-color: #008000\">33.7193</span>  \n",
       "  ▁          <span style=\"color: #008000; text-decoration-color: #008000\">0.0340</span>    <span style=\"color: #008000; text-decoration-color: #008000\">26.2205</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.2369</span>     <span style=\"color: #008000; text-decoration-color: #008000\">4.0255</span>    <span style=\"color: #008000; text-decoration-color: #008000\">24.5529</span>    <span style=\"color: #008000; text-decoration-color: #008000\">27.5611</span>    <span style=\"color: #008000; text-decoration-color: #008000\">18.0878</span>  \n",
       "  3          <span style=\"color: #008000; text-decoration-color: #008000\">0.0337</span>    <span style=\"color: #008000; text-decoration-color: #008000\">28.9997</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.2823</span>     <span style=\"color: #008000; text-decoration-color: #008000\">5.0299</span>    <span style=\"color: #008000; text-decoration-color: #008000\">29.9640</span>    <span style=\"color: #008000; text-decoration-color: #008000\">59.1844</span>    <span style=\"color: #008000; text-decoration-color: #008000\">32.6388</span>  \n",
       "  ▁apples    <span style=\"color: #008000; text-decoration-color: #008000\">0.0632</span>    <span style=\"color: #008000; text-decoration-color: #008000\">66.3430</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.4825</span>     <span style=\"color: #008000; text-decoration-color: #008000\">9.2379</span>    <span style=\"color: #008000; text-decoration-color: #008000\">43.1779</span>    <span style=\"color: #008000; text-decoration-color: #008000\">48.0342</span>    <span style=\"color: #008000; text-decoration-color: #008000\">40.1680</span>  \n",
       "  ▁and       <span style=\"color: #008000; text-decoration-color: #008000\">0.0410</span>    <span style=\"color: #008000; text-decoration-color: #008000\">35.8075</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.3035</span>     <span style=\"color: #008000; text-decoration-color: #008000\">5.8184</span>    <span style=\"color: #008000; text-decoration-color: #008000\">28.4838</span>    <span style=\"color: #008000; text-decoration-color: #008000\">29.5762</span>    <span style=\"color: #008000; text-decoration-color: #008000\">19.1980</span>  \n",
       "  ▁John      <span style=\"color: #008000; text-decoration-color: #008000\">0.0550</span>    <span style=\"color: #008000; text-decoration-color: #008000\">36.0833</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.4106</span>     <span style=\"color: #008000; text-decoration-color: #008000\">7.2349</span>    <span style=\"color: #008000; text-decoration-color: #008000\">24.1436</span>    <span style=\"color: #008000; text-decoration-color: #008000\">21.1167</span>    <span style=\"color: #008000; text-decoration-color: #008000\">19.9228</span>  \n",
       "  ▁has       <span style=\"color: #008000; text-decoration-color: #008000\">0.0457</span>    <span style=\"color: #008000; text-decoration-color: #008000\">33.2646</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.3197</span>     <span style=\"color: #008000; text-decoration-color: #008000\">6.0864</span>    <span style=\"color: #008000; text-decoration-color: #008000\">27.5308</span>    <span style=\"color: #008000; text-decoration-color: #008000\">27.2985</span>    <span style=\"color: #008000; text-decoration-color: #008000\">20.2047</span>  \n",
       "  ▁          <span style=\"color: #008000; text-decoration-color: #008000\">0.0342</span>    <span style=\"color: #008000; text-decoration-color: #008000\">18.9863</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.2035</span>     <span style=\"color: #008000; text-decoration-color: #008000\">3.3157</span>    <span style=\"color: #008000; text-decoration-color: #008000\">20.6291</span>    <span style=\"color: #008000; text-decoration-color: #008000\">17.4662</span>    <span style=\"color: #008000; text-decoration-color: #008000\">14.2829</span>  \n",
       "  5          <span style=\"color: #008000; text-decoration-color: #008000\">0.0317</span>    <span style=\"color: #008000; text-decoration-color: #008000\">25.6511</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.2331</span>     <span style=\"color: #008000; text-decoration-color: #008000\">3.8841</span>    <span style=\"color: #008000; text-decoration-color: #008000\">17.9062</span>    <span style=\"color: #008000; text-decoration-color: #008000\">18.8697</span>    <span style=\"color: #008000; text-decoration-color: #008000\">20.8133</span>  \n",
       "  ▁oranges   <span style=\"color: #008000; text-decoration-color: #008000\">0.0598</span>    <span style=\"color: #008000; text-decoration-color: #008000\">44.3548</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.3668</span>     <span style=\"color: #008000; text-decoration-color: #008000\">6.8322</span>    <span style=\"color: #008000; text-decoration-color: #008000\">33.3165</span>    <span style=\"color: #008000; text-decoration-color: #008000\">41.9764</span>    <span style=\"color: #008000; text-decoration-color: #008000\">27.7359</span>  \n",
       "  .          <span style=\"color: #008000; text-decoration-color: #008000\">0.0435</span>    <span style=\"color: #008000; text-decoration-color: #008000\">32.0238</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.3410</span>     <span style=\"color: #008000; text-decoration-color: #008000\">6.2816</span>    <span style=\"color: #008000; text-decoration-color: #008000\">24.9733</span>    <span style=\"color: #008000; text-decoration-color: #008000\">17.1649</span>    <span style=\"color: #008000; text-decoration-color: #008000\">18.4730</span>  \n",
       "  ▁How       <span style=\"color: #008000; text-decoration-color: #008000\">0.0459</span>    <span style=\"color: #008000; text-decoration-color: #008000\">36.6674</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.3877</span>     <span style=\"color: #008000; text-decoration-color: #008000\">7.0877</span>    <span style=\"color: #008000; text-decoration-color: #008000\">29.7497</span>    <span style=\"color: #008000; text-decoration-color: #008000\">22.6589</span>    <span style=\"color: #008000; text-decoration-color: #008000\">22.1241</span>  \n",
       "  ▁many      <span style=\"color: #008000; text-decoration-color: #008000\">0.0470</span>    <span style=\"color: #008000; text-decoration-color: #008000\">36.2684</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.3264</span>     <span style=\"color: #008000; text-decoration-color: #008000\">6.4464</span>    <span style=\"color: #008000; text-decoration-color: #008000\">33.1295</span>    <span style=\"color: #008000; text-decoration-color: #008000\">30.9442</span>    <span style=\"color: #008000; text-decoration-color: #008000\">27.3642</span>  \n",
       "  ▁apples    <span style=\"color: #008000; text-decoration-color: #008000\">0.0716</span>    <span style=\"color: #008000; text-decoration-color: #008000\">43.8999</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.3341</span>     <span style=\"color: #008000; text-decoration-color: #008000\">7.3740</span>    <span style=\"color: #008000; text-decoration-color: #008000\">46.5694</span>    <span style=\"color: #008000; text-decoration-color: #008000\">51.8812</span>    <span style=\"color: #008000; text-decoration-color: #008000\">32.5115</span>  \n",
       "  ▁does      <span style=\"color: #008000; text-decoration-color: #008000\">0.0323</span>    <span style=\"color: #008000; text-decoration-color: #008000\">33.0140</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.2630</span>     <span style=\"color: #008000; text-decoration-color: #008000\">4.6643</span>    <span style=\"color: #008000; text-decoration-color: #008000\">23.4358</span>    <span style=\"color: #008000; text-decoration-color: #008000\">19.6655</span>    <span style=\"color: #008000; text-decoration-color: #008000\">14.9890</span>  \n",
       "  ▁Sam       <span style=\"color: #008000; text-decoration-color: #008000\">0.0409</span>    <span style=\"color: #008000; text-decoration-color: #008000\">27.3865</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.3168</span>     <span style=\"color: #008000; text-decoration-color: #008000\">4.7084</span>    <span style=\"color: #008000; text-decoration-color: #008000\">28.0450</span>    <span style=\"color: #008000; text-decoration-color: #008000\">26.5090</span>    <span style=\"color: #008000; text-decoration-color: #008000\">17.9461</span>  \n",
       "  ▁have      <span style=\"color: #008000; text-decoration-color: #008000\">0.0649</span>    <span style=\"color: #008000; text-decoration-color: #008000\">34.1126</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.2902</span>     <span style=\"color: #008000; text-decoration-color: #008000\">6.5851</span>    <span style=\"color: #008000; text-decoration-color: #008000\">35.5255</span>    <span style=\"color: #008000; text-decoration-color: #008000\">22.1672</span>    <span style=\"color: #008000; text-decoration-color: #008000\">23.9016</span>  \n",
       "  ?          <span style=\"color: #008000; text-decoration-color: #008000\">0.1469</span>    <span style=\"color: #008000; text-decoration-color: #008000\">59.1601</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.5917</span>    <span style=\"color: #008000; text-decoration-color: #008000\">11.7581</span>    <span style=\"color: #008000; text-decoration-color: #008000\">68.4922</span>    <span style=\"color: #008000; text-decoration-color: #008000\">40.9595</span>    <span style=\"color: #008000; text-decoration-color: #008000\">41.5126</span>  \n",
       "             <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>   <span style=\"color: #008000; text-decoration-color: #008000\">150.9977</span>    <span style=\"color: #008000; text-decoration-color: #008000\">1.2861</span>    <span style=\"color: #008000; text-decoration-color: #008000\">19.3280</span>    <span style=\"color: #008000; text-decoration-color: #008000\">45.8290</span>    <span style=\"color: #008000; text-decoration-color: #008000\">32.2954</span>    <span style=\"color: #008000; text-decoration-color: #008000\">41.1756</span>  \n",
       "                                                                                      \n",
       "                                                                                      \n",
       "  The        <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>     <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>    <span style=\"color: #008000; text-decoration-color: #008000\">1.2992</span>     <span style=\"color: #008000; text-decoration-color: #008000\">8.3744</span>    <span style=\"color: #008000; text-decoration-color: #008000\">20.3927</span>     <span style=\"color: #008000; text-decoration-color: #008000\">8.5602</span>    <span style=\"color: #008000; text-decoration-color: #008000\">16.5280</span>  \n",
       "  ▁answer    <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>     <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>    <span style=\"color: #008000; text-decoration-color: #008000\">27.1139</span>    <span style=\"color: #008000; text-decoration-color: #008000\">38.1941</span>    <span style=\"color: #008000; text-decoration-color: #008000\">18.5473</span>    <span style=\"color: #008000; text-decoration-color: #008000\">29.7542</span>  \n",
       "  ▁is        <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>     <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>     <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>    <span style=\"color: #008000; text-decoration-color: #008000\">94.0511</span>    <span style=\"color: #008000; text-decoration-color: #008000\">27.8277</span>    <span style=\"color: #008000; text-decoration-color: #008000\">47.6029</span>  \n",
       "  ▁          <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>     <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>     <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>     <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>    <span style=\"color: #008000; text-decoration-color: #008000\">57.5610</span>    <span style=\"color: #008000; text-decoration-color: #008000\">14.0093</span>  \n",
       "  3          <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>     <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>     <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>     <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>     <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>    <span style=\"color: #008000; text-decoration-color: #008000\">48.1393</span>  \n",
       "  .          <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>     <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>    <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>     <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>     <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>     <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>     <span style=\"color: #008000; text-decoration-color: #008000\">0.0000</span>  \n",
       "                                                                                      \n",
       "</pre>\n"
      ],
      "text/plain": [
       "                                                                                      \n",
       " \u001b[1m          \u001b[0m \u001b[1m \u001b[0m\u001b[1m      \u001b[0m\u001b[1m \u001b[0m \u001b[1m          \u001b[0m \u001b[1m         \u001b[0m \u001b[1m          \u001b[0m \u001b[1m          \u001b[0m \u001b[1m          \u001b[0m \u001b[1m          \u001b[0m \n",
       " \u001b[1m          \u001b[0m \u001b[1m \u001b[0m\u001b[1m      \u001b[0m\u001b[1m \u001b[0m \u001b[1m          \u001b[0m \u001b[1m         \u001b[0m \u001b[1m          \u001b[0m \u001b[1m          \u001b[0m \u001b[1m          \u001b[0m \u001b[1m          \u001b[0m \n",
       " \u001b[1m \u001b[0m\u001b[1m        \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m      \u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m     The\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m▁answer\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m     ▁is\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m       ▁\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m       3\u001b[0m\u001b[1m \u001b[0m \u001b[1m \u001b[0m\u001b[1m       .\u001b[0m\u001b[1m \u001b[0m \n",
       " ──────────────────────────────────────────────────────────────────────────────────── \n",
       "  <bos>      \u001b[32m0.7480\u001b[0m   \u001b[31m827.3525\u001b[0m    \u001b[32m2.4268\u001b[0m   \u001b[33m231.6353\u001b[0m   \u001b[31m551.3010\u001b[0m   \u001b[31m460.8656\u001b[0m   \u001b[31m488.2078\u001b[0m  \n",
       "  Sam        \u001b[32m0.1883\u001b[0m   \u001b[32m185.5456\u001b[0m    \u001b[32m1.8554\u001b[0m    \u001b[32m38.9216\u001b[0m   \u001b[32m104.9274\u001b[0m    \u001b[32m24.2310\u001b[0m   \u001b[32m122.5083\u001b[0m  \n",
       "  ▁has       \u001b[32m0.0620\u001b[0m    \u001b[32m57.6189\u001b[0m    \u001b[32m0.4312\u001b[0m     \u001b[32m8.8093\u001b[0m    \u001b[32m41.0717\u001b[0m    \u001b[32m38.5860\u001b[0m    \u001b[32m33.7193\u001b[0m  \n",
       "  ▁          \u001b[32m0.0340\u001b[0m    \u001b[32m26.2205\u001b[0m    \u001b[32m0.2369\u001b[0m     \u001b[32m4.0255\u001b[0m    \u001b[32m24.5529\u001b[0m    \u001b[32m27.5611\u001b[0m    \u001b[32m18.0878\u001b[0m  \n",
       "  3          \u001b[32m0.0337\u001b[0m    \u001b[32m28.9997\u001b[0m    \u001b[32m0.2823\u001b[0m     \u001b[32m5.0299\u001b[0m    \u001b[32m29.9640\u001b[0m    \u001b[32m59.1844\u001b[0m    \u001b[32m32.6388\u001b[0m  \n",
       "  ▁apples    \u001b[32m0.0632\u001b[0m    \u001b[32m66.3430\u001b[0m    \u001b[32m0.4825\u001b[0m     \u001b[32m9.2379\u001b[0m    \u001b[32m43.1779\u001b[0m    \u001b[32m48.0342\u001b[0m    \u001b[32m40.1680\u001b[0m  \n",
       "  ▁and       \u001b[32m0.0410\u001b[0m    \u001b[32m35.8075\u001b[0m    \u001b[32m0.3035\u001b[0m     \u001b[32m5.8184\u001b[0m    \u001b[32m28.4838\u001b[0m    \u001b[32m29.5762\u001b[0m    \u001b[32m19.1980\u001b[0m  \n",
       "  ▁John      \u001b[32m0.0550\u001b[0m    \u001b[32m36.0833\u001b[0m    \u001b[32m0.4106\u001b[0m     \u001b[32m7.2349\u001b[0m    \u001b[32m24.1436\u001b[0m    \u001b[32m21.1167\u001b[0m    \u001b[32m19.9228\u001b[0m  \n",
       "  ▁has       \u001b[32m0.0457\u001b[0m    \u001b[32m33.2646\u001b[0m    \u001b[32m0.3197\u001b[0m     \u001b[32m6.0864\u001b[0m    \u001b[32m27.5308\u001b[0m    \u001b[32m27.2985\u001b[0m    \u001b[32m20.2047\u001b[0m  \n",
       "  ▁          \u001b[32m0.0342\u001b[0m    \u001b[32m18.9863\u001b[0m    \u001b[32m0.2035\u001b[0m     \u001b[32m3.3157\u001b[0m    \u001b[32m20.6291\u001b[0m    \u001b[32m17.4662\u001b[0m    \u001b[32m14.2829\u001b[0m  \n",
       "  5          \u001b[32m0.0317\u001b[0m    \u001b[32m25.6511\u001b[0m    \u001b[32m0.2331\u001b[0m     \u001b[32m3.8841\u001b[0m    \u001b[32m17.9062\u001b[0m    \u001b[32m18.8697\u001b[0m    \u001b[32m20.8133\u001b[0m  \n",
       "  ▁oranges   \u001b[32m0.0598\u001b[0m    \u001b[32m44.3548\u001b[0m    \u001b[32m0.3668\u001b[0m     \u001b[32m6.8322\u001b[0m    \u001b[32m33.3165\u001b[0m    \u001b[32m41.9764\u001b[0m    \u001b[32m27.7359\u001b[0m  \n",
       "  .          \u001b[32m0.0435\u001b[0m    \u001b[32m32.0238\u001b[0m    \u001b[32m0.3410\u001b[0m     \u001b[32m6.2816\u001b[0m    \u001b[32m24.9733\u001b[0m    \u001b[32m17.1649\u001b[0m    \u001b[32m18.4730\u001b[0m  \n",
       "  ▁How       \u001b[32m0.0459\u001b[0m    \u001b[32m36.6674\u001b[0m    \u001b[32m0.3877\u001b[0m     \u001b[32m7.0877\u001b[0m    \u001b[32m29.7497\u001b[0m    \u001b[32m22.6589\u001b[0m    \u001b[32m22.1241\u001b[0m  \n",
       "  ▁many      \u001b[32m0.0470\u001b[0m    \u001b[32m36.2684\u001b[0m    \u001b[32m0.3264\u001b[0m     \u001b[32m6.4464\u001b[0m    \u001b[32m33.1295\u001b[0m    \u001b[32m30.9442\u001b[0m    \u001b[32m27.3642\u001b[0m  \n",
       "  ▁apples    \u001b[32m0.0716\u001b[0m    \u001b[32m43.8999\u001b[0m    \u001b[32m0.3341\u001b[0m     \u001b[32m7.3740\u001b[0m    \u001b[32m46.5694\u001b[0m    \u001b[32m51.8812\u001b[0m    \u001b[32m32.5115\u001b[0m  \n",
       "  ▁does      \u001b[32m0.0323\u001b[0m    \u001b[32m33.0140\u001b[0m    \u001b[32m0.2630\u001b[0m     \u001b[32m4.6643\u001b[0m    \u001b[32m23.4358\u001b[0m    \u001b[32m19.6655\u001b[0m    \u001b[32m14.9890\u001b[0m  \n",
       "  ▁Sam       \u001b[32m0.0409\u001b[0m    \u001b[32m27.3865\u001b[0m    \u001b[32m0.3168\u001b[0m     \u001b[32m4.7084\u001b[0m    \u001b[32m28.0450\u001b[0m    \u001b[32m26.5090\u001b[0m    \u001b[32m17.9461\u001b[0m  \n",
       "  ▁have      \u001b[32m0.0649\u001b[0m    \u001b[32m34.1126\u001b[0m    \u001b[32m0.2902\u001b[0m     \u001b[32m6.5851\u001b[0m    \u001b[32m35.5255\u001b[0m    \u001b[32m22.1672\u001b[0m    \u001b[32m23.9016\u001b[0m  \n",
       "  ?          \u001b[32m0.1469\u001b[0m    \u001b[32m59.1601\u001b[0m    \u001b[32m0.5917\u001b[0m    \u001b[32m11.7581\u001b[0m    \u001b[32m68.4922\u001b[0m    \u001b[32m40.9595\u001b[0m    \u001b[32m41.5126\u001b[0m  \n",
       "             \u001b[32m0.0000\u001b[0m   \u001b[32m150.9977\u001b[0m    \u001b[32m1.2861\u001b[0m    \u001b[32m19.3280\u001b[0m    \u001b[32m45.8290\u001b[0m    \u001b[32m32.2954\u001b[0m    \u001b[32m41.1756\u001b[0m  \n",
       "                                                                                      \n",
       "                                                                                      \n",
       "  The        \u001b[32m0.0000\u001b[0m     \u001b[32m0.0000\u001b[0m    \u001b[32m1.2992\u001b[0m     \u001b[32m8.3744\u001b[0m    \u001b[32m20.3927\u001b[0m     \u001b[32m8.5602\u001b[0m    \u001b[32m16.5280\u001b[0m  \n",
       "  ▁answer    \u001b[32m0.0000\u001b[0m     \u001b[32m0.0000\u001b[0m    \u001b[32m0.0000\u001b[0m    \u001b[32m27.1139\u001b[0m    \u001b[32m38.1941\u001b[0m    \u001b[32m18.5473\u001b[0m    \u001b[32m29.7542\u001b[0m  \n",
       "  ▁is        \u001b[32m0.0000\u001b[0m     \u001b[32m0.0000\u001b[0m    \u001b[32m0.0000\u001b[0m     \u001b[32m0.0000\u001b[0m    \u001b[32m94.0511\u001b[0m    \u001b[32m27.8277\u001b[0m    \u001b[32m47.6029\u001b[0m  \n",
       "  ▁          \u001b[32m0.0000\u001b[0m     \u001b[32m0.0000\u001b[0m    \u001b[32m0.0000\u001b[0m     \u001b[32m0.0000\u001b[0m     \u001b[32m0.0000\u001b[0m    \u001b[32m57.5610\u001b[0m    \u001b[32m14.0093\u001b[0m  \n",
       "  3          \u001b[32m0.0000\u001b[0m     \u001b[32m0.0000\u001b[0m    \u001b[32m0.0000\u001b[0m     \u001b[32m0.0000\u001b[0m     \u001b[32m0.0000\u001b[0m     \u001b[32m0.0000\u001b[0m    \u001b[32m48.1393\u001b[0m  \n",
       "  .          \u001b[32m0.0000\u001b[0m     \u001b[32m0.0000\u001b[0m    \u001b[32m0.0000\u001b[0m     \u001b[32m0.0000\u001b[0m     \u001b[32m0.0000\u001b[0m     \u001b[32m0.0000\u001b[0m     \u001b[32m0.0000\u001b[0m  \n",
       "                                                                                      \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attributor = LocalLLMAttributor(model=model, embeddings=embeddings, tokenizer=tokenizer)\n",
    "attr_scores, token_ids = attributor.compute_attributions(\n",
    "    input_string=\"Sam has 3 apples and John has 5 oranges. How many apples does Sam have?\",\n",
    "    generation_length=7,\n",
    ")\n",
    "\n",
    "attributor.print_attributions(\n",
    "    word_list=tokenizer.convert_ids_to_tokens(token_ids),\n",
    "    attr_scores=attr_scores,\n",
    "    token_ids=token_ids,\n",
    "    generation_length=7,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the attribution table, we see some sensible values associated with the output token `3` which is the correct answer:\n",
    "\n",
    "|Input Token        | Output Token | Attribution |\n",
    "|-------------------|--------------|-------------|\n",
    "|`3`                | `3`          |  59.1844    |\n",
    "|`apples` (First)   | `3`          |  48.0342    |\n",
    "|`apples` (Second)  | `3`          |  51.8812    |\n",
    "|`5`                | `3`          |  18.8697    |\n",
    "\n",
    "There are high attributions for the input tokens that are strongly associated with the correct answer and low attribution for the input token `5` which would be the incorrect answer. There is something strange about some attribution values in this table for the output token `3`, notably the one associated with the`oranges` and both `Sam` input tokens.\n",
    "\n",
    "|Input Token        | Output Token | Attribution |\n",
    "|-------------------|--------------|-------------|\n",
    "|`oranges`          | `3`          |  41.9764    |\n",
    "|`Sam` (First)      | `3`          |  24.2310    |\n",
    "|`Sam` (Second)     | `3`          |  26.5090    |\n",
    "\n",
    "This value for the `oranges` token is surprisingly close to the input tokens associated with the correct answer input tokens and also quite a bit larger than the values associated with the incorrect answer input tokens. The values for both the `Sam` tokens however are surprisingly low and ideally should be higher since the model answers the question in Scenario 1 correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above analysis, we can come up with two hypotheses about model behaviour that could be potential causes for the failure in Scenario 2:\n",
    "\n",
    "__Hypothesis 1__: Gemma-2B pays undue attention on the `oranges` tokens when asked about `apples` in Scenario 1 pointing to some confusion around which fruit is being asked about in the question.\n",
    "\n",
    "__Hypothesis 2__: Gemma-2B doesn't associate `apples` strongly with `Sam` in Scenario 1 which causes the model to output the incorrect answer in Scenario 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we show a case study where attribution helps us generate hypotheses to understand model failures. There is much work to be done in this space and we encourage readers to further inspect more model failures and validate the usefulness of attribution. We welcome proposals for features and communication regarding experiments with our library to be sent to `hello@leap-labs.com`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
