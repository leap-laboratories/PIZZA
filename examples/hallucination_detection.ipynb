{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training and validation CSV files\n",
    "train_df = pd.read_csv(\"examples/data/train_hc_dataset_small_20240915_044230.csv\")\n",
    "val_df = pd.read_csv(\"examples/data/val_hc_dataset_small_20240915_044230.csv\")\n",
    "\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_data(df):\n",
    "    # Parse 'prob_diff_values' into list of floats\n",
    "    df[\"prob_diff_sequence\"] = df[\"prob_diff_values\"].apply(ast.literal_eval)\n",
    "    df[\"cosine_sequence\"] = df[\"cosine_values\"].apply(ast.literal_eval)\n",
    "\n",
    "    # Encode 'verdict' into numerical labels\n",
    "    le = LabelEncoder()\n",
    "    df[\"verdict_label\"] = le.fit_transform(df[\"verdict\"])\n",
    "\n",
    "    # Pad sequences to the maximum length\n",
    "    max_prob_diff_len = df[\"prob_diff_sequence\"].apply(len).max()\n",
    "    max_cosine_len = df[\"cosine_sequence\"].apply(len).max()\n",
    "\n",
    "    def pad_sequence(seq, max_len):\n",
    "        return seq + [0] * (max_len - len(seq))\n",
    "\n",
    "    prob_diff_sequence_padded = np.array(\n",
    "        df[\"prob_diff_sequence\"].apply(lambda x: pad_sequence(x, max_prob_diff_len))\n",
    "    )\n",
    "    prob_diff_sequence_padded_df = pd.DataFrame(\n",
    "        prob_diff_sequence_padded.tolist(),\n",
    "        columns=[f\"prob_diff_sequence_padded_{i}\" for i in range(max_prob_diff_len)],\n",
    "    )\n",
    "\n",
    "    cosine_sequence_padded = np.array(\n",
    "        df[\"cosine_sequence\"].apply(lambda x: pad_sequence(x, max_cosine_len))\n",
    "    )\n",
    "\n",
    "    cosine_sequence_padded_df = pd.DataFrame(\n",
    "        cosine_sequence_padded.tolist(),\n",
    "        columns=[f\"cosine_sequence_padded_{i}\" for i in range(max_cosine_len)],\n",
    "    )\n",
    "\n",
    "    df = pd.concat([df, prob_diff_sequence_padded_df, cosine_sequence_padded_df], axis=1)\n",
    "\n",
    "    # Compute statistical features\n",
    "    df[\"prob_diff_mean\"] = df[\"prob_diff_sequence\"].apply(np.mean)\n",
    "    df[\"prob_diff_std\"] = df[\"prob_diff_sequence\"].apply(np.std)\n",
    "    df[\"prob_diff_max\"] = df[\"prob_diff_sequence\"].apply(np.max)\n",
    "    df[\"prob_diff_min\"] = df[\"prob_diff_sequence\"].apply(np.min)\n",
    "    df[\"prob_diff_median\"] = df[\"prob_diff_sequence\"].apply(np.median)\n",
    "    df[\"prob_diff_q25\"] = df[\"prob_diff_sequence\"].apply(lambda x: np.percentile(x, 25))\n",
    "    df[\"prob_diff_q75\"] = df[\"prob_diff_sequence\"].apply(lambda x: np.percentile(x, 75))\n",
    "\n",
    "    df[\"cosine_mean\"] = df[\"cosine_sequence\"].apply(np.mean)\n",
    "    df[\"cosine_std\"] = df[\"cosine_sequence\"].apply(np.std)\n",
    "    df[\"cosine_max\"] = df[\"cosine_sequence\"].apply(np.max)\n",
    "    df[\"cosine_min\"] = df[\"cosine_sequence\"].apply(np.min)\n",
    "    df[\"cosine_median\"] = df[\"cosine_sequence\"].apply(np.median)\n",
    "    df[\"cosine_q25\"] = df[\"cosine_sequence\"].apply(lambda x: np.percentile(x, 25))\n",
    "    df[\"cosine_q75\"] = df[\"cosine_sequence\"].apply(lambda x: np.percentile(x, 75))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Preprocess training and validation data\n",
    "train_df = preprocess_data(train_df)\n",
    "val_df = preprocess_data(val_df)\n",
    "\n",
    "# Define feature columns\n",
    "prob_diff_features = [\n",
    "    \"prob_diff_mean\",\n",
    "    \"prob_diff_std\",\n",
    "    \"prob_diff_max\",\n",
    "    \"prob_diff_min\",\n",
    "    \"prob_diff_median\",\n",
    "    \"prob_diff_q25\",\n",
    "    \"prob_diff_q75\",\n",
    "]\n",
    "\n",
    "cosine_features = [\n",
    "    \"cosine_mean\",\n",
    "    \"cosine_std\",\n",
    "    \"cosine_max\",\n",
    "    \"cosine_min\",\n",
    "    \"cosine_median\",\n",
    "    \"cosine_q25\",\n",
    "    \"cosine_q75\",\n",
    "]\n",
    "\n",
    "prob_diff_sequence_cols = [c for c in train_df.columns if \"prob_diff_sequence_padded\" in c]\n",
    "cosine_sequence_cols = [c for c in train_df.columns if \"cosine_sequence_padded\" in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define datasets\n",
    "class HallucinationDataset(Dataset):\n",
    "    def __init__(self, df, X_columns, y_column):\n",
    "        self.sequences = df[X_columns].values\n",
    "        self.labels = df[y_column].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = torch.tensor(self.sequences[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return sequence, label\n",
    "\n",
    "    def sample(self, n):\n",
    "        idx = np.random.choice(len(self), n, replace=False)\n",
    "        return self.__getitem__(idx)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=20):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for sequences, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for sequences, labels in val_loader:\n",
    "                outputs = model(sequences)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{epochs}, Train Loss: {running_loss/len(train_loader)}, Val Loss: {val_loss/len(val_loader)}, Val Accuracy: {100 * correct/total}%\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets and dataloaders\n",
    "batch_size = 128\n",
    "\n",
    "train_dataset = HallucinationDataset(train_df, prob_diff_sequence_cols, \"verdict_label\")\n",
    "val_dataset = HallucinationDataset(val_df, prob_diff_sequence_cols, \"verdict_label\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "stats_train_dataset = HallucinationDataset(train_df, prob_diff_features, \"verdict_label\")\n",
    "stats_val_dataset = HallucinationDataset(val_df, prob_diff_features, \"verdict_label\")\n",
    "\n",
    "stats_train_loader = DataLoader(stats_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "stats_val_loader = DataLoader(stats_val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearClassifier(\n",
      "  (weights): Linear(in_features=7, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jessicarumbelow/PIZZA/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.6687743466811119, Val Loss: 0.6492811921817153, Val Accuracy: 66.14090431125132%\n",
      "Epoch 2/20, Train Loss: 0.6385472736645155, Val Loss: 0.6337828075707849, Val Accuracy: 67.09896015889707%\n",
      "Epoch 3/20, Train Loss: 0.6273689262345113, Val Loss: 0.6273951623866807, Val Accuracy: 66.94707325622151%\n",
      "Epoch 4/20, Train Loss: 0.6216344142676423, Val Loss: 0.6242937988309718, Val Accuracy: 67.1223273746933%\n",
      "Epoch 5/20, Train Loss: 0.6184488386555291, Val Loss: 0.6221186104995101, Val Accuracy: 67.4261011800444%\n",
      "Epoch 6/20, Train Loss: 0.615961301736054, Val Loss: 0.6203376081452441, Val Accuracy: 67.44946839584064%\n",
      "Epoch 7/20, Train Loss: 0.6142521393145615, Val Loss: 0.618952736925723, Val Accuracy: 67.58967169061806%\n",
      "Epoch 8/20, Train Loss: 0.6122803048514501, Val Loss: 0.6178583778552155, Val Accuracy: 67.65977333800677%\n",
      "Epoch 9/20, Train Loss: 0.6108433632379949, Val Loss: 0.6180352266155072, Val Accuracy: 67.32094870896132%\n",
      "Epoch 10/20, Train Loss: 0.6097758079803041, Val Loss: 0.6159213220895227, Val Accuracy: 68.22058651711649%\n",
      "Epoch 11/20, Train Loss: 0.608774559180624, Val Loss: 0.6151216381521367, Val Accuracy: 68.33742259609768%\n",
      "Epoch 12/20, Train Loss: 0.6075945422373105, Val Loss: 0.6141762831317845, Val Accuracy: 68.26732094870896%\n",
      "Epoch 13/20, Train Loss: 0.6067050900070452, Val Loss: 0.6134958004773553, Val Accuracy: 68.32573898819956%\n",
      "Epoch 14/20, Train Loss: 0.6057891855935682, Val Loss: 0.613038224960441, Val Accuracy: 68.41920785138451%\n",
      "Epoch 15/20, Train Loss: 0.6051413552443868, Val Loss: 0.6130008101463318, Val Accuracy: 67.87007828017292%\n",
      "Epoch 16/20, Train Loss: 0.6044845059194278, Val Loss: 0.6117321057106132, Val Accuracy: 68.51267671456947%\n",
      "Epoch 17/20, Train Loss: 0.6038355934773392, Val Loss: 0.6115635889234827, Val Accuracy: 68.6061455777544%\n",
      "Epoch 18/20, Train Loss: 0.6034116320344, Val Loss: 0.6111187690229558, Val Accuracy: 68.58277836195818%\n",
      "Epoch 19/20, Train Loss: 0.6026490980463478, Val Loss: 0.61178876437358, Val Accuracy: 67.68314055380301%\n",
      "Epoch 20/20, Train Loss: 0.6021791003292722, Val Loss: 0.6103841199803708, Val Accuracy: 68.4776258908751%\n"
     ]
    }
   ],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "\n",
    "        self.weights = nn.Linear(input_shape, output_shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.weights(x)\n",
    "\n",
    "\n",
    "X_sample, y_sample = stats_train_dataset.sample(1)\n",
    "\n",
    "# Instantiate the model\n",
    "model = LinearClassifier(input_shape=X_sample.shape[-1], output_shape=2)\n",
    "print(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, stats_train_loader, stats_val_loader, criterion, optimizer, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearClassifier(\n",
      "  (weights): Linear(in_features=296, out_features=2, bias=True)\n",
      ")\n",
      "Epoch 1/20, Train Loss: 0.6572112893853577, Val Loss: 0.6484712313360242, Val Accuracy: 63.290103984110296%\n",
      "Epoch 2/20, Train Loss: 0.6435949336817336, Val Loss: 0.6453771675700573, Val Accuracy: 63.07979904194415%\n",
      "Epoch 3/20, Train Loss: 0.6402308342282864, Val Loss: 0.6442113024974937, Val Accuracy: 63.00969739455544%\n",
      "Epoch 4/20, Train Loss: 0.6381710660304123, Val Loss: 0.644451797898136, Val Accuracy: 61.28052342563384%\n",
      "Epoch 5/20, Train Loss: 0.6370994323312981, Val Loss: 0.6435991676885691, Val Accuracy: 62.191844841687114%\n",
      "Epoch 6/20, Train Loss: 0.6360173678193481, Val Loss: 0.6427220288497298, Val Accuracy: 63.02138100245356%\n",
      "Epoch 7/20, Train Loss: 0.6351033400568328, Val Loss: 0.6425115328226516, Val Accuracy: 63.00969739455544%\n",
      "Epoch 8/20, Train Loss: 0.634503760051318, Val Loss: 0.6422100983448883, Val Accuracy: 62.78770884449118%\n",
      "Epoch 9/20, Train Loss: 0.6340355765666061, Val Loss: 0.6441241529450488, Val Accuracy: 63.6055613973595%\n",
      "Epoch 10/20, Train Loss: 0.6334260824923863, Val Loss: 0.6416240267789186, Val Accuracy: 62.986330178759204%\n",
      "Epoch 11/20, Train Loss: 0.6328251162312062, Val Loss: 0.6413432871227833, Val Accuracy: 62.68255637340811%\n",
      "Epoch 12/20, Train Loss: 0.6326334811587191, Val Loss: 0.6414698335661817, Val Accuracy: 63.02138100245356%\n",
      "Epoch 13/20, Train Loss: 0.6322298244345341, Val Loss: 0.6415889547831977, Val Accuracy: 63.418623670989604%\n",
      "Epoch 14/20, Train Loss: 0.6318811868904998, Val Loss: 0.6417331913513924, Val Accuracy: 63.32515480780465%\n",
      "Epoch 15/20, Train Loss: 0.6316039142690503, Val Loss: 0.6413779445548555, Val Accuracy: 63.208318728823464%\n",
      "Epoch 16/20, Train Loss: 0.6316069863384886, Val Loss: 0.6418845947109052, Val Accuracy: 62.986330178759204%\n",
      "Epoch 17/20, Train Loss: 0.6313756671074634, Val Loss: 0.6411257186042729, Val Accuracy: 62.96296296296296%\n",
      "Epoch 18/20, Train Loss: 0.6310639271408703, Val Loss: 0.6409478690197219, Val Accuracy: 62.7409744128987%\n",
      "Epoch 19/20, Train Loss: 0.6310171349365824, Val Loss: 0.6406177464705795, Val Accuracy: 62.635821941815635%\n",
      "Epoch 20/20, Train Loss: 0.6307544309182228, Val Loss: 0.6408354929134027, Val Accuracy: 63.044748218249794%\n"
     ]
    }
   ],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "\n",
    "        self.weights = nn.Linear(input_shape, output_shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.weights(x)\n",
    "\n",
    "\n",
    "X_sample, y_sample = train_dataset.sample(1)\n",
    "\n",
    "# Instantiate the model\n",
    "model = LinearClassifier(input_shape=X_sample.shape[-1], output_shape=2)\n",
    "print(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=296, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=33, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=33, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 1/20, Train Loss: 0.6092315034804938, Val Loss: 0.6036146369887821, Val Accuracy: 68.5360439303657%\n",
      "Epoch 2/20, Train Loss: 0.5789172593104481, Val Loss: 0.5842429308304146, Val Accuracy: 70.19511625189858%\n",
      "Epoch 3/20, Train Loss: 0.5663356641587269, Val Loss: 0.5818815427039986, Val Accuracy: 70.0782801729174%\n",
      "Epoch 4/20, Train Loss: 0.5611574817368913, Val Loss: 0.5735514570972813, Val Accuracy: 70.97791798107255%\n",
      "Epoch 5/20, Train Loss: 0.5535842984786873, Val Loss: 0.5735278345310866, Val Accuracy: 70.77929664680454%\n",
      "Epoch 6/20, Train Loss: 0.5483545504926101, Val Loss: 0.5691738064164547, Val Accuracy: 70.91949994158196%\n",
      "Epoch 7/20, Train Loss: 0.5448523640888443, Val Loss: 0.5755075692240872, Val Accuracy: 70.21848346769482%\n",
      "Epoch 8/20, Train Loss: 0.541896814312546, Val Loss: 0.5734232634083548, Val Accuracy: 70.93118354948008%\n",
      "Epoch 9/20, Train Loss: 0.5408188627768995, Val Loss: 0.5662883299945006, Val Accuracy: 71.39852786540483%\n",
      "Epoch 10/20, Train Loss: 0.5367156629142843, Val Loss: 0.5712152268014737, Val Accuracy: 70.46383923355532%\n",
      "Epoch 11/20, Train Loss: 0.5358726148185812, Val Loss: 0.5702921679215645, Val Accuracy: 70.96623437317443%\n",
      "Epoch 12/20, Train Loss: 0.5324456735230311, Val Loss: 0.567059105456765, Val Accuracy: 71.41021147330295%\n",
      "Epoch 13/20, Train Loss: 0.5302562983506739, Val Loss: 0.5731399590844539, Val Accuracy: 71.23495735483117%\n",
      "Epoch 14/20, Train Loss: 0.5311358486633956, Val Loss: 0.5645145678253316, Val Accuracy: 71.41021147330295%\n",
      "Epoch 15/20, Train Loss: 0.5277853147666342, Val Loss: 0.5680299363474348, Val Accuracy: 70.98960158897067%\n",
      "Epoch 16/20, Train Loss: 0.5259015246778087, Val Loss: 0.5705793913620621, Val Accuracy: 71.81913774973712%\n",
      "Epoch 17/20, Train Loss: 0.5253275680183853, Val Loss: 0.5725274261698794, Val Accuracy: 71.53873116018227%\n",
      "Epoch 18/20, Train Loss: 0.5239983531026882, Val Loss: 0.5712243536515023, Val Accuracy: 71.64388363126534%\n",
      "Epoch 19/20, Train Loss: 0.5218219462894064, Val Loss: 0.5686105957894183, Val Accuracy: 71.16485570744246%\n",
      "Epoch 20/20, Train Loss: 0.5202503032950373, Val Loss: 0.5778741976663248, Val Accuracy: 70.73256221521206%\n"
     ]
    }
   ],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape, breadth, depth, interpolate=True):\n",
    "        super(SimpleNN, self).__init__()\n",
    "\n",
    "        self.layers = []\n",
    "\n",
    "        self.layers.append(nn.Linear(input_shape, breadth))\n",
    "        self.layers.append(nn.ReLU())\n",
    "\n",
    "        if interpolate:\n",
    "            change_rate = int((breadth - output_shape) / max(1, depth))\n",
    "            layer_sizes = [breadth - change_rate * i for i in range(depth)]\n",
    "\n",
    "            for l in layer_sizes[:-1]:\n",
    "                self.layers.append(nn.Linear(l, l - change_rate))\n",
    "                self.layers.append(nn.ReLU())\n",
    "\n",
    "            self.layers.append(nn.Linear(layer_sizes[-1], output_shape))\n",
    "\n",
    "        else:\n",
    "            for _ in range(depth - 1):\n",
    "                self.layers.append(nn.Linear(breadth, breadth))\n",
    "                self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Linear(breadth, output_shape))\n",
    "\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "X_sample, y_sample = train_dataset.sample(1)\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleNN(input_shape=X_sample.shape[-1], output_shape=2, breadth=64, depth=2)\n",
    "print(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleConvNet(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv1d(1, 7, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): ReLU()\n",
      "    (2): Conv1d(7, 7, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (fc): Linear(in_features=2072, out_features=2, bias=True)\n",
      ")\n",
      "Epoch 1/20, Train Loss: 0.6954296228711697, Val Loss: 0.6936510943654758, Val Accuracy: 49.66701717490361%\n",
      "Epoch 2/20, Train Loss: 0.690574354368218, Val Loss: 0.6647087691435173, Val Accuracy: 61.595980838883044%\n",
      "Epoch 3/20, Train Loss: 0.6384924934145718, Val Loss: 0.623130173825506, Val Accuracy: 66.7484519219535%\n",
      "Epoch 4/20, Train Loss: 0.6160503472381396, Val Loss: 0.6093393501950733, Val Accuracy: 68.03364879074658%\n",
      "Epoch 5/20, Train Loss: 0.6026682851140591, Val Loss: 0.6176767847431239, Val Accuracy: 65.4515714452623%\n",
      "Epoch 6/20, Train Loss: 0.5929928994997377, Val Loss: 0.5993154189035074, Val Accuracy: 67.62472251431242%\n",
      "Epoch 7/20, Train Loss: 0.5885375080702131, Val Loss: 0.5972273698938426, Val Accuracy: 68.37247341979203%\n",
      "Epoch 8/20, Train Loss: 0.5853150735085614, Val Loss: 0.593900032452683, Val Accuracy: 69.29547844374343%\n",
      "Epoch 9/20, Train Loss: 0.5845480999977292, Val Loss: 0.5948194928133665, Val Accuracy: 68.81645051992055%\n",
      "Epoch 10/20, Train Loss: 0.5823021757500366, Val Loss: 0.594492637844228, Val Accuracy: 69.27211122794719%\n",
      "Epoch 11/20, Train Loss: 0.582495131589824, Val Loss: 0.5921865065595997, Val Accuracy: 69.26042762004907%\n",
      "Epoch 12/20, Train Loss: 0.5830123505111416, Val Loss: 0.5939921207392393, Val Accuracy: 69.24874401215095%\n",
      "Epoch 13/20, Train Loss: 0.5817861797471926, Val Loss: 0.5925363769282156, Val Accuracy: 69.4590489543171%\n",
      "Epoch 14/20, Train Loss: 0.5806249087204749, Val Loss: 0.5919092444341574, Val Accuracy: 69.3422128753359%\n",
      "Epoch 15/20, Train Loss: 0.5816825666652728, Val Loss: 0.5917719820542122, Val Accuracy: 69.22537679635471%\n",
      "Epoch 16/20, Train Loss: 0.5799160537034145, Val Loss: 0.5924300649272862, Val Accuracy: 69.71608832807571%\n",
      "Epoch 17/20, Train Loss: 0.5792336603346813, Val Loss: 0.5942498427718433, Val Accuracy: 69.2136931884566%\n",
      "Epoch 18/20, Train Loss: 0.5785210352598853, Val Loss: 0.5989485004944588, Val Accuracy: 68.26732094870896%\n",
      "Epoch 19/20, Train Loss: 0.5793528082288898, Val Loss: 0.593673342866684, Val Accuracy: 69.50578338590957%\n",
      "Epoch 20/20, Train Loss: 0.5777997420581114, Val Loss: 0.5931178869596169, Val Accuracy: 69.26042762004907%\n"
     ]
    }
   ],
   "source": [
    "class SimpleConvNet(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape, breadth, depth):\n",
    "        super(SimpleConvNet, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        in_channels = input_shape[0]\n",
    "        ks = 3\n",
    "        num_filters = breadth // (ks) ** 2\n",
    "\n",
    "        # Add convolutional layers that maintain the input dimension\n",
    "        for _ in range(depth):\n",
    "            layers.append(nn.Conv1d(in_channels, num_filters, kernel_size=3, padding=1))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_channels = num_filters\n",
    "\n",
    "        self.conv_layers = nn.Sequential(*layers)\n",
    "\n",
    "        # Calculate the size of the flattened output\n",
    "        conv_output_shape = input_shape[1] * input_shape[2] * num_filters\n",
    "\n",
    "        # Fully connected layer for classification\n",
    "        self.fc = nn.Linear(conv_output_shape, output_shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "X_sample, y_sample = train_dataset.sample(1)\n",
    "input_shape = X_sample.unsqueeze(1).shape\n",
    "# Instantiate the model\n",
    "model = SimpleConvNet(input_shape=input_shape, output_shape=2, breadth=64, depth=2)\n",
    "print(model)\n",
    "\n",
    "# Rest of the setup remains the same\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleRNN(\n",
      "  (rnn): RNN(296, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n",
      "Epoch 1/20, Train Loss: 0.6395679517876949, Val Loss: 0.6163862142989884, Val Accuracy: 67.82334384858044%\n",
      "Epoch 2/20, Train Loss: 0.6051266115366645, Val Loss: 0.6087914416149481, Val Accuracy: 67.48451921953499%\n",
      "Epoch 3/20, Train Loss: 0.5881137239063247, Val Loss: 0.5899924920566046, Val Accuracy: 69.03843906998482%\n",
      "Epoch 4/20, Train Loss: 0.5845461357304979, Val Loss: 0.5895802405343127, Val Accuracy: 69.54083420960393%\n",
      "Epoch 5/20, Train Loss: 0.5731739075654566, Val Loss: 0.5823497325181961, Val Accuracy: 69.62261946489076%\n",
      "Epoch 6/20, Train Loss: 0.5731813173437323, Val Loss: 0.6078525699341475, Val Accuracy: 66.72508470615726%\n",
      "Epoch 7/20, Train Loss: 0.5693853175179641, Val Loss: 0.5830367970377651, Val Accuracy: 69.90302605444562%\n",
      "Epoch 8/20, Train Loss: 0.5650829755952942, Val Loss: 0.5819652878971242, Val Accuracy: 70.52225727304592%\n",
      "Epoch 9/20, Train Loss: 0.563574261036042, Val Loss: 0.5874068841560564, Val Accuracy: 69.17864236476224%\n",
      "Epoch 10/20, Train Loss: 0.5630482790807798, Val Loss: 0.5919307788361364, Val Accuracy: 68.76971608832808%\n",
      "Epoch 11/20, Train Loss: 0.5588295963700749, Val Loss: 0.5755151083220297, Val Accuracy: 70.90781633368384%\n",
      "Epoch 12/20, Train Loss: 0.5584132026705108, Val Loss: 0.5771117110305758, Val Accuracy: 70.17174903610236%\n",
      "Epoch 13/20, Train Loss: 0.5561562207379567, Val Loss: 0.5828158713098782, Val Accuracy: 70.19511625189858%\n",
      "Epoch 14/20, Train Loss: 0.5565948115397932, Val Loss: 0.5859682809061079, Val Accuracy: 69.7745063675663%\n",
      "Epoch 15/20, Train Loss: 0.5534656494216346, Val Loss: 0.5789505790418653, Val Accuracy: 69.62261946489076%\n",
      "Epoch 16/20, Train Loss: 0.5538431042765343, Val Loss: 0.5838027663195311, Val Accuracy: 69.24874401215095%\n",
      "Epoch 17/20, Train Loss: 0.5529956545185122, Val Loss: 0.5799022975252636, Val Accuracy: 69.78618997546442%\n",
      "Epoch 18/20, Train Loss: 0.5513268522438574, Val Loss: 0.5830410329708412, Val Accuracy: 70.28858511508354%\n",
      "Epoch 19/20, Train Loss: 0.5524252742656822, Val Loss: 0.5912072035803724, Val Accuracy: 68.5360439303657%\n",
      "Epoch 20/20, Train Loss: 0.5509687527822322, Val Loss: 0.5791677665799412, Val Accuracy: 70.40542119406473%\n"
     ]
    }
   ],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape, breadth, depth):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_shape,\n",
    "            hidden_size=breadth,\n",
    "            num_layers=depth,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(breadth, output_shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        _, h_n = self.rnn(x)\n",
    "        out = self.fc(h_n[-1])\n",
    "        return out\n",
    "\n",
    "\n",
    "# Sample a batch from the dataset\n",
    "X_sample, y_sample = train_dataset.sample(1)\n",
    "input_shape = X_sample.shape[-1]  # Extract the number of features\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleRNN(input_shape=input_shape, output_shape=2, breadth=64, depth=2)\n",
    "print(model)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
