{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training and validation CSV files\n",
    "train_df = pd.read_csv(\"examples/data/train_hc_dataset_small_20240915_044230.csv\")\n",
    "val_df = pd.read_csv(\"examples/data/val_hc_dataset_small_20240915_044230.csv\")\n",
    "\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_data(df):\n",
    "    # Parse 'prob_diff_values' into list of floats\n",
    "    df[\"prob_diff_sequence\"] = df[\"prob_diff_values\"].apply(ast.literal_eval)\n",
    "    df[\"cosine_sequence\"] = df[\"cosine_values\"].apply(ast.literal_eval)\n",
    "\n",
    "    # Encode 'verdict' into numerical labels\n",
    "    le = LabelEncoder()\n",
    "    df[\"verdict_label\"] = le.fit_transform(df[\"verdict\"])\n",
    "\n",
    "    # Pad sequences to the maximum length\n",
    "    max_prob_diff_len = df[\"prob_diff_sequence\"].apply(len).max()\n",
    "    max_cosine_len = df[\"cosine_sequence\"].apply(len).max()\n",
    "\n",
    "    def pad_sequence(seq, max_len):\n",
    "        return seq + [0] * (max_len - len(seq))\n",
    "\n",
    "    prob_diff_sequence_padded = np.array(\n",
    "        df[\"prob_diff_sequence\"].apply(lambda x: pad_sequence(x, max_prob_diff_len))\n",
    "    )\n",
    "    prob_diff_sequence_padded_df = pd.DataFrame(\n",
    "        prob_diff_sequence_padded.tolist(),\n",
    "        columns=[f\"prob_diff_sequence_padded_{i}\" for i in range(max_prob_diff_len)],\n",
    "    )\n",
    "\n",
    "    cosine_sequence_padded = np.array(\n",
    "        df[\"cosine_sequence\"].apply(lambda x: pad_sequence(x, max_cosine_len))\n",
    "    )\n",
    "\n",
    "    cosine_sequence_padded_df = pd.DataFrame(\n",
    "        cosine_sequence_padded.tolist(),\n",
    "        columns=[f\"cosine_sequence_padded_{i}\" for i in range(max_cosine_len)],\n",
    "    )\n",
    "\n",
    "    df = pd.concat([df, prob_diff_sequence_padded_df, cosine_sequence_padded_df], axis=1)\n",
    "\n",
    "    # Compute statistical features\n",
    "    df[\"prob_diff_mean\"] = df[\"prob_diff_sequence\"].apply(np.mean)\n",
    "    df[\"prob_diff_std\"] = df[\"prob_diff_sequence\"].apply(np.std)\n",
    "    df[\"prob_diff_max\"] = df[\"prob_diff_sequence\"].apply(np.max)\n",
    "    df[\"prob_diff_min\"] = df[\"prob_diff_sequence\"].apply(np.min)\n",
    "    df[\"prob_diff_median\"] = df[\"prob_diff_sequence\"].apply(np.median)\n",
    "    df[\"prob_diff_q25\"] = df[\"prob_diff_sequence\"].apply(lambda x: np.percentile(x, 25))\n",
    "    df[\"prob_diff_q75\"] = df[\"prob_diff_sequence\"].apply(lambda x: np.percentile(x, 75))\n",
    "\n",
    "    df[\"cosine_mean\"] = df[\"cosine_sequence\"].apply(np.mean)\n",
    "    df[\"cosine_std\"] = df[\"cosine_sequence\"].apply(np.std)\n",
    "    df[\"cosine_max\"] = df[\"cosine_sequence\"].apply(np.max)\n",
    "    df[\"cosine_min\"] = df[\"cosine_sequence\"].apply(np.min)\n",
    "    df[\"cosine_median\"] = df[\"cosine_sequence\"].apply(np.median)\n",
    "    df[\"cosine_q25\"] = df[\"cosine_sequence\"].apply(lambda x: np.percentile(x, 25))\n",
    "    df[\"cosine_q75\"] = df[\"cosine_sequence\"].apply(lambda x: np.percentile(x, 75))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Preprocess training and validation data\n",
    "train_df = preprocess_data(train_df)\n",
    "val_df = preprocess_data(val_df)\n",
    "\n",
    "# Define feature columns\n",
    "prob_diff_features = [\n",
    "    \"prob_diff_mean\",\n",
    "    \"prob_diff_std\",\n",
    "    \"prob_diff_max\",\n",
    "    \"prob_diff_min\",\n",
    "    \"prob_diff_median\",\n",
    "    \"prob_diff_q25\",\n",
    "    \"prob_diff_q75\",\n",
    "]\n",
    "\n",
    "cosine_features = [\n",
    "    \"cosine_mean\",\n",
    "    \"cosine_std\",\n",
    "    \"cosine_max\",\n",
    "    \"cosine_min\",\n",
    "    \"cosine_median\",\n",
    "    \"cosine_q25\",\n",
    "    \"cosine_q75\",\n",
    "]\n",
    "\n",
    "prob_diff_sequence_cols = [c for c in train_df.columns if \"prob_diff_sequence_padded\" in c]\n",
    "cosine_sequence_cols = [c for c in train_df.columns if \"cosine_sequence_padded\" in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define datasets\n",
    "class HallucinationDataset(Dataset):\n",
    "    def __init__(self, df, X_columns, y_column):\n",
    "        self.sequences = df[X_columns].values\n",
    "        self.labels = df[y_column].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = torch.tensor(self.sequences[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return sequence, label\n",
    "\n",
    "    def sample(self, n):\n",
    "        idx = np.random.choice(len(self), n, replace=False)\n",
    "        return self.__getitem__(idx)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=20):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for sequences, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for sequences, labels in val_loader:\n",
    "                outputs = model(sequences)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{epochs}, Train Loss: {running_loss/len(train_loader)}, Val Loss: {val_loss/len(val_loader)}, Val Accuracy: {100 * correct/total}%\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare datasets and dataloaders\n",
    "batch_size = 128\n",
    "\n",
    "train_dataset = HallucinationDataset(train_df, prob_diff_sequence_cols, \"verdict_label\")\n",
    "val_dataset = HallucinationDataset(val_df, prob_diff_sequence_cols, \"verdict_label\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "stats_train_dataset = HallucinationDataset(train_df, prob_diff_features, \"verdict_label\")\n",
    "stats_val_dataset = HallucinationDataset(val_df, prob_diff_features, \"verdict_label\")\n",
    "\n",
    "stats_train_loader = DataLoader(stats_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "stats_val_loader = DataLoader(stats_val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearClassifier(\n",
      "  (weights): Linear(in_features=7, out_features=2, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jessicarumbelow/PIZZA/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.6732943815223137, Val Loss: 0.651663503540096, Val Accuracy: 66.60824862717607%\n",
      "Epoch 2/20, Train Loss: 0.6401324962853362, Val Loss: 0.6352753087655821, Val Accuracy: 67.06390933520271%\n",
      "Epoch 3/20, Train Loss: 0.6282372298158801, Val Loss: 0.628185863370326, Val Accuracy: 66.64329945087043%\n",
      "Epoch 4/20, Train Loss: 0.6224274630198663, Val Loss: 0.6247008349468459, Val Accuracy: 67.22747984577637%\n",
      "Epoch 5/20, Train Loss: 0.6190712720539436, Val Loss: 0.6224232142540946, Val Accuracy: 67.34431592475757%\n",
      "Epoch 6/20, Train Loss: 0.6162990508161389, Val Loss: 0.6207018903831938, Val Accuracy: 67.49620282743311%\n",
      "Epoch 7/20, Train Loss: 0.6141786255549975, Val Loss: 0.6201649219242494, Val Accuracy: 67.81166024068233%\n",
      "Epoch 8/20, Train Loss: 0.6125273568947428, Val Loss: 0.617969688639712, Val Accuracy: 67.58967169061806%\n",
      "Epoch 9/20, Train Loss: 0.6114429092202576, Val Loss: 0.6171151982314551, Val Accuracy: 68.12711765393153%\n",
      "Epoch 10/20, Train Loss: 0.6099601480070613, Val Loss: 0.6165940690396438, Val Accuracy: 68.18553569342212%\n",
      "Epoch 11/20, Train Loss: 0.6089637888347642, Val Loss: 0.6150200447039817, Val Accuracy: 67.95186353545975%\n",
      "Epoch 12/20, Train Loss: 0.6077489198021623, Val Loss: 0.6142814021501968, Val Accuracy: 68.25563734081084%\n",
      "Epoch 13/20, Train Loss: 0.6069925739529819, Val Loss: 0.6138524168462896, Val Accuracy: 68.40752424348639%\n",
      "Epoch 14/20, Train Loss: 0.6061620497396576, Val Loss: 0.6140362119941569, Val Accuracy: 68.55941114616193%\n",
      "Epoch 15/20, Train Loss: 0.6054995883687883, Val Loss: 0.6125434254532429, Val Accuracy: 68.34910620399579%\n",
      "Epoch 16/20, Train Loss: 0.6046306145549332, Val Loss: 0.6130968439045237, Val Accuracy: 68.45425867507886%\n",
      "Epoch 17/20, Train Loss: 0.6040901109384365, Val Loss: 0.6115261930138317, Val Accuracy: 68.5360439303657%\n",
      "Epoch 18/20, Train Loss: 0.6034629268196008, Val Loss: 0.6108999327937169, Val Accuracy: 68.46594228297698%\n",
      "Epoch 19/20, Train Loss: 0.602882039905106, Val Loss: 0.6113127625700253, Val Accuracy: 67.96354714335787%\n",
      "Epoch 20/20, Train Loss: 0.6024921244817742, Val Loss: 0.6102149611088767, Val Accuracy: 68.55941114616193%\n"
     ]
    }
   ],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "\n",
    "        self.weights = nn.Linear(input_shape, output_shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.weights(x)\n",
    "\n",
    "\n",
    "X_sample, y_sample = stats_train_dataset.sample(1)\n",
    "\n",
    "# Instantiate the model\n",
    "model = LinearClassifier(input_shape=X_sample.shape[-1], output_shape=2)\n",
    "print(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, stats_train_loader, stats_val_loader, criterion, optimizer, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearClassifier(\n",
      "  (weights): Linear(in_features=296, out_features=2, bias=True)\n",
      ")\n",
      "Epoch 1/20, Train Loss: 0.6566719300245523, Val Loss: 0.6494654737301727, Val Accuracy: 64.35331230283911%\n",
      "Epoch 2/20, Train Loss: 0.6435410907340152, Val Loss: 0.6456847653460147, Val Accuracy: 63.488725318378314%\n",
      "Epoch 3/20, Train Loss: 0.6401916831859703, Val Loss: 0.6451910445049628, Val Accuracy: 61.52587919149433%\n",
      "Epoch 4/20, Train Loss: 0.6381766074716789, Val Loss: 0.6438956923449217, Val Accuracy: 63.03306461035168%\n",
      "Epoch 5/20, Train Loss: 0.6367162473723612, Val Loss: 0.6433325102969781, Val Accuracy: 62.76434162869494%\n",
      "Epoch 6/20, Train Loss: 0.6358812482060281, Val Loss: 0.6426762191217337, Val Accuracy: 62.76434162869494%\n",
      "Epoch 7/20, Train Loss: 0.635043342226053, Val Loss: 0.6440338127648653, Val Accuracy: 63.82754994742376%\n",
      "Epoch 8/20, Train Loss: 0.634314519141365, Val Loss: 0.6422778986283203, Val Accuracy: 62.97464657086108%\n",
      "Epoch 9/20, Train Loss: 0.6340894236073473, Val Loss: 0.6426579351745435, Val Accuracy: 61.52587919149433%\n",
      "Epoch 10/20, Train Loss: 0.6333466407567134, Val Loss: 0.6415610580301997, Val Accuracy: 62.97464657086108%\n",
      "Epoch 11/20, Train Loss: 0.6330361261388263, Val Loss: 0.6438792152191276, Val Accuracy: 63.65229582895198%\n",
      "Epoch 12/20, Train Loss: 0.6327693426557877, Val Loss: 0.6430601277458134, Val Accuracy: 63.39525645519336%\n",
      "Epoch 13/20, Train Loss: 0.6324713672691149, Val Loss: 0.6414055330539817, Val Accuracy: 62.916228531370486%\n",
      "Epoch 14/20, Train Loss: 0.6317670734143565, Val Loss: 0.6430492801452751, Val Accuracy: 63.068115434046035%\n",
      "Epoch 15/20, Train Loss: 0.6319450296045884, Val Loss: 0.6412458504313854, Val Accuracy: 62.08669237060404%\n",
      "Epoch 16/20, Train Loss: 0.6316657017740569, Val Loss: 0.6422317165936997, Val Accuracy: 63.056431826147914%\n",
      "Epoch 17/20, Train Loss: 0.6313623604344708, Val Loss: 0.6421905707067518, Val Accuracy: 63.02138100245356%\n",
      "Epoch 18/20, Train Loss: 0.6312963473950333, Val Loss: 0.6410258069856843, Val Accuracy: 63.11484986563851%\n",
      "Epoch 19/20, Train Loss: 0.631394915570517, Val Loss: 0.6404257025291671, Val Accuracy: 62.33204813646454%\n",
      "Epoch 20/20, Train Loss: 0.6308795628117901, Val Loss: 0.641001834798215, Val Accuracy: 62.95127935506484%\n"
     ]
    }
   ],
   "source": [
    "class LinearClassifier(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape):\n",
    "        super(LinearClassifier, self).__init__()\n",
    "\n",
    "        self.weights = nn.Linear(input_shape, output_shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.weights(x)\n",
    "\n",
    "\n",
    "X_sample, y_sample = train_dataset.sample(1)\n",
    "\n",
    "# Instantiate the model\n",
    "model = LinearClassifier(input_shape=X_sample.shape[-1], output_shape=2)\n",
    "print(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=296, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=65, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=65, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Epoch 1/20, Train Loss: 0.6113929350744501, Val Loss: 0.593511347895238, Val Accuracy: 69.57588503329828%\n",
      "Epoch 2/20, Train Loss: 0.5734497975126357, Val Loss: 0.5830924259637719, Val Accuracy: 70.25353429138919%\n",
      "Epoch 3/20, Train Loss: 0.5633064828205518, Val Loss: 0.582635097539247, Val Accuracy: 70.38205397826849%\n",
      "Epoch 4/20, Train Loss: 0.5578267854426552, Val Loss: 0.5778400353086528, Val Accuracy: 71.30505900221989%\n",
      "Epoch 5/20, Train Loss: 0.5487795044935824, Val Loss: 0.5925013367364655, Val Accuracy: 68.99170463839233%\n",
      "Epoch 6/20, Train Loss: 0.546453017021965, Val Loss: 0.570870851069244, Val Accuracy: 71.316742610118%\n",
      "Epoch 7/20, Train Loss: 0.5418298436336763, Val Loss: 0.5669747994684461, Val Accuracy: 71.29337539432177%\n",
      "Epoch 8/20, Train Loss: 0.5384613710411629, Val Loss: 0.5710682207301482, Val Accuracy: 71.16485570744246%\n",
      "Epoch 9/20, Train Loss: 0.5375391994423109, Val Loss: 0.5682767552893553, Val Accuracy: 71.22327374693305%\n",
      "Epoch 10/20, Train Loss: 0.5359937092009532, Val Loss: 0.5654478742560344, Val Accuracy: 71.21159013903494%\n",
      "Epoch 11/20, Train Loss: 0.532143127764755, Val Loss: 0.5699239987045971, Val Accuracy: 71.48031312069168%\n",
      "Epoch 12/20, Train Loss: 0.5324125275847226, Val Loss: 0.566599680639025, Val Accuracy: 71.43357868909919%\n",
      "Epoch 13/20, Train Loss: 0.5283740070756413, Val Loss: 0.5641139492170134, Val Accuracy: 71.44526229699731%\n",
      "Epoch 14/20, Train Loss: 0.5283840693885165, Val Loss: 0.5617213182484926, Val Accuracy: 71.35179343381236%\n",
      "Epoch 15/20, Train Loss: 0.5261362581293982, Val Loss: 0.5650490391165462, Val Accuracy: 70.97791798107255%\n",
      "Epoch 16/20, Train Loss: 0.5227400212840461, Val Loss: 0.56574280204168, Val Accuracy: 71.15317209954434%\n",
      "Epoch 17/20, Train Loss: 0.5220802839477686, Val Loss: 0.568398565721156, Val Accuracy: 71.01296880476691%\n",
      "Epoch 18/20, Train Loss: 0.5208451124987377, Val Loss: 0.5660008163149677, Val Accuracy: 71.6205164154691%\n",
      "Epoch 19/20, Train Loss: 0.5186872267416107, Val Loss: 0.5680350421079948, Val Accuracy: 71.59714919967286%\n",
      "Epoch 20/20, Train Loss: 0.5174498194788658, Val Loss: 0.5736976616195778, Val Accuracy: 70.73256221521206%\n"
     ]
    }
   ],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape, breadth, depth, interpolate=True):\n",
    "        super(SimpleNN, self).__init__()\n",
    "\n",
    "        self.layers = []\n",
    "\n",
    "        self.layers.append(nn.Linear(input_shape, breadth))\n",
    "        self.layers.append(nn.ReLU())\n",
    "\n",
    "        if interpolate:\n",
    "            change_rate = int((breadth - output_shape) / max(1, depth))\n",
    "            layer_sizes = [breadth - change_rate * i for i in range(depth)]\n",
    "\n",
    "            for l in layer_sizes[:-1]:\n",
    "                self.layers.append(nn.Linear(l, l - change_rate))\n",
    "                self.layers.append(nn.ReLU())\n",
    "\n",
    "            self.layers.append(nn.Linear(layer_sizes[-1], output_shape))\n",
    "\n",
    "        else:\n",
    "            for _ in range(depth - 1):\n",
    "                self.layers.append(nn.Linear(breadth, breadth))\n",
    "                self.layers.append(nn.ReLU())\n",
    "            self.layers.append(nn.Linear(breadth, output_shape))\n",
    "\n",
    "        self.layers = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "X_sample, y_sample = train_dataset.sample(1)\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleNN(input_shape=X_sample.shape[-1], output_shape=2, breadth=64, depth=2)\n",
    "print(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleConvNet(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv1d(1, 3, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc): Linear(in_features=888, out_features=2, bias=True)\n",
      ")\n",
      "Epoch 1/20, Train Loss: 0.6475355827245589, Val Loss: 0.6315331699243233, Val Accuracy: 65.72029442691904%\n",
      "Epoch 2/20, Train Loss: 0.6312549106552877, Val Loss: 0.6283989219523188, Val Accuracy: 66.10585348755696%\n",
      "Epoch 3/20, Train Loss: 0.628067748485205, Val Loss: 0.6282000784108888, Val Accuracy: 66.60824862717607%\n",
      "Epoch 4/20, Train Loss: 0.6257177230626216, Val Loss: 0.6252434538371527, Val Accuracy: 66.42131090080616%\n",
      "Epoch 5/20, Train Loss: 0.6229434118250409, Val Loss: 0.6250542884887155, Val Accuracy: 65.95396658488141%\n",
      "Epoch 6/20, Train Loss: 0.6210374681223104, Val Loss: 0.6265836106752282, Val Accuracy: 66.3161584297231%\n",
      "Epoch 7/20, Train Loss: 0.6189656464838674, Val Loss: 0.6219079067457968, Val Accuracy: 66.3161584297231%\n",
      "Epoch 8/20, Train Loss: 0.6172021056961092, Val Loss: 0.621704331974485, Val Accuracy: 66.51477976399111%\n",
      "Epoch 9/20, Train Loss: 0.6154770472530643, Val Loss: 0.6196380226469752, Val Accuracy: 65.89554854539081%\n",
      "Epoch 10/20, Train Loss: 0.6129679723358973, Val Loss: 0.6185734476616134, Val Accuracy: 66.38626007711181%\n",
      "Epoch 11/20, Train Loss: 0.6118378196663099, Val Loss: 0.6160730379286097, Val Accuracy: 66.76013552985162%\n",
      "Epoch 12/20, Train Loss: 0.6100575906012703, Val Loss: 0.615502816527637, Val Accuracy: 66.64329945087043%\n",
      "Epoch 13/20, Train Loss: 0.6079946748688497, Val Loss: 0.6125075323368186, Val Accuracy: 67.07559294310083%\n",
      "Epoch 14/20, Train Loss: 0.6059183616494928, Val Loss: 0.6111424031542309, Val Accuracy: 66.5615141955836%\n",
      "Epoch 15/20, Train Loss: 0.6031995308245712, Val Loss: 0.6165269824551113, Val Accuracy: 66.78350274564785%\n",
      "Epoch 16/20, Train Loss: 0.6030394752138163, Val Loss: 0.6121559414400983, Val Accuracy: 65.93059936908517%\n",
      "Epoch 17/20, Train Loss: 0.602017349951257, Val Loss: 0.6098472424852315, Val Accuracy: 67.48451921953499%\n",
      "Epoch 18/20, Train Loss: 0.5999893418197468, Val Loss: 0.6065196717408166, Val Accuracy: 67.51957004322935%\n",
      "Epoch 19/20, Train Loss: 0.6000323037221197, Val Loss: 0.6063096770154897, Val Accuracy: 67.08727655099895%\n",
      "Epoch 20/20, Train Loss: 0.5998954031088833, Val Loss: 0.6053776227271379, Val Accuracy: 67.91681271176539%\n"
     ]
    }
   ],
   "source": [
    "class SimpleConvNet(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape, breadth, depth):\n",
    "        super(SimpleConvNet, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        in_channels = input_shape[0]\n",
    "        ks = 3\n",
    "        num_filters = breadth // (ks) ** 2\n",
    "\n",
    "        # Add convolutional layers that maintain the input dimension\n",
    "        for _ in range(depth):\n",
    "            layers.append(nn.Conv1d(in_channels, num_filters, kernel_size=3, padding=1))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_channels = num_filters\n",
    "\n",
    "        self.conv_layers = nn.Sequential(*layers)\n",
    "\n",
    "        # Calculate the size of the flattened output\n",
    "        conv_output_shape = input_shape[1] * input_shape[2] * num_filters\n",
    "\n",
    "        # Fully connected layer for classification\n",
    "        self.fc = nn.Linear(conv_output_shape, output_shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the output\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "X_sample, y_sample = train_dataset.sample(1)\n",
    "input_shape = X_sample.unsqueeze(1).shape\n",
    "# Instantiate the model\n",
    "model = SimpleConvNet(input_shape=input_shape, output_shape=2, breadth=64, depth=2)\n",
    "print(model)\n",
    "\n",
    "# Rest of the setup remains the same\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleRNN(\n",
      "  (rnn): RNN(296, 64, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n",
      "Epoch 1/20, Train Loss: 0.6388754970014351, Val Loss: 0.6359873498553661, Val Accuracy: 62.75265802079682%\n",
      "Epoch 2/20, Train Loss: 0.6066626740115907, Val Loss: 0.6138586891231252, Val Accuracy: 67.26253066947073%\n",
      "Epoch 3/20, Train Loss: 0.5937342344435499, Val Loss: 0.6046077875948664, Val Accuracy: 68.173852085524%\n",
      "Epoch 4/20, Train Loss: 0.5843781340275711, Val Loss: 0.6009919881375868, Val Accuracy: 67.06390933520271%\n",
      "Epoch 5/20, Train Loss: 0.5772432902340213, Val Loss: 0.588089759225276, Val Accuracy: 69.42399813062273%\n",
      "Epoch 6/20, Train Loss: 0.5693036013406745, Val Loss: 0.582054913711192, Val Accuracy: 69.99649491763057%\n",
      "Epoch 7/20, Train Loss: 0.5633276172717754, Val Loss: 0.5852905173799885, Val Accuracy: 69.84460801495501%\n",
      "Epoch 8/20, Train Loss: 0.5638035929765824, Val Loss: 0.5800513820861702, Val Accuracy: 70.08996378081551%\n",
      "Epoch 9/20, Train Loss: 0.5586064665102651, Val Loss: 0.5796285798745369, Val Accuracy: 69.8913424465475%\n",
      "Epoch 10/20, Train Loss: 0.5602642580419139, Val Loss: 0.5794805862120728, Val Accuracy: 69.8329244070569%\n",
      "Epoch 11/20, Train Loss: 0.5587663413884814, Val Loss: 0.6146343279240737, Val Accuracy: 67.11064376679519%\n",
      "Epoch 12/20, Train Loss: 0.5583818325924771, Val Loss: 0.5751801079778529, Val Accuracy: 70.17174903610236%\n",
      "Epoch 13/20, Train Loss: 0.5511205559636391, Val Loss: 0.5764098196332135, Val Accuracy: 71.21159013903494%\n",
      "Epoch 14/20, Train Loss: 0.5539377856152252, Val Loss: 0.5831332480284706, Val Accuracy: 69.85629162285313%\n",
      "Epoch 15/20, Train Loss: 0.5521446816911002, Val Loss: 0.5790079958848099, Val Accuracy: 70.72087860731394%\n",
      "Epoch 16/20, Train Loss: 0.5513771116221923, Val Loss: 0.5789028873194509, Val Accuracy: 71.24664096272929%\n",
      "Epoch 17/20, Train Loss: 0.5490471606858298, Val Loss: 0.5729752263026451, Val Accuracy: 71.14148849164621%\n",
      "Epoch 18/20, Train Loss: 0.5513571929266524, Val Loss: 0.5808609148014837, Val Accuracy: 70.40542119406473%\n",
      "Epoch 19/20, Train Loss: 0.5487571157866793, Val Loss: 0.5759710327013216, Val Accuracy: 70.53394088094403%\n",
      "Epoch 20/20, Train Loss: 0.5485598558775857, Val Loss: 0.574314206393797, Val Accuracy: 70.70919499941581%\n"
     ]
    }
   ],
   "source": [
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_shape, output_shape, breadth, depth):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=input_shape,\n",
    "            hidden_size=breadth,\n",
    "            num_layers=depth,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.fc = nn.Linear(breadth, output_shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        _, h_n = self.rnn(x)\n",
    "        out = self.fc(h_n[-1])\n",
    "        return out\n",
    "\n",
    "\n",
    "# Sample a batch from the dataset\n",
    "X_sample, y_sample = train_dataset.sample(1)\n",
    "input_shape = X_sample.shape[-1]  # Extract the number of features\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleRNN(input_shape=input_shape, output_shape=2, breadth=64, depth=2)\n",
    "print(model)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
